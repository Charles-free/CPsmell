<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<unit xmlns="http://www.srcML.org/srcML/src" xmlns:cpp="http://www.srcML.org/srcML/cpp" revision="1.0.0" language="C++" filename="python_variable_methods.cpp" hash="7bf3412ab3d233145b3a326e7f04dbf4ce749b0b"><cpp:define>#<cpp:directive>define</cpp:directive> <cpp:macro><name>TORCH_ASSERT_ONLY_METHOD_OPERATORS</name></cpp:macro></cpp:define>
<comment type="line">// ${generated_comment}</comment>

<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;Python.h&gt;</cpp:file></cpp:include>

<comment type="line">// Undefine the copysign macro so that at::copysign works as intended with MSVC</comment>
<comment type="line">// https://github.com/python/cpython/blob/c60394c7fc9cc09b16e9675a3eeb5844b6d8523f/PC/pyconfig.h#L196</comment>
<cpp:ifdef>#<cpp:directive>ifdef</cpp:directive> <name>_MSC_VER</name></cpp:ifdef>
<cpp:undef>#<cpp:directive>undef</cpp:directive> <name>copysign</name></cpp:undef>
<cpp:endif>#<cpp:directive>endif</cpp:directive></cpp:endif> <comment type="line">// _MSC_VER</comment>

<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/DynamicTypes.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/Exceptions.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/Size.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/autograd/generated/VariableType.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/autograd/python_variable.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/autograd/utils/python_arg_parsing.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/autograd/utils/error_messages.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/autograd/utils/wrap_outputs.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/jit/frontend/tracer.h"</cpp:file></cpp:include>
<cpp:ifdef>#<cpp:directive>ifdef</cpp:directive> <name>USE_CUDA</name></cpp:ifdef>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/cuda/Event.h"</cpp:file></cpp:include>
<cpp:endif>#<cpp:directive>endif</cpp:directive></cpp:endif>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/utils/cuda_lazy_init.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/utils/object_ptr.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/utils/pycfunction_helpers.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/utils/python_arg_parser.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/utils/python_numbers.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/utils/python_strings.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/utils/python_tuples.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/utils/tensor_apply.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/utils/tensor_list.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/utils/tensor_new.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/utils/tensor_numpy.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/utils/tensor_types.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/utils/structseq.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"torch/csrc/autograd/python_return_types.h"</cpp:file></cpp:include>

<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;ATen/core/Tensor.h&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;ATen/FuncTorchTLS.h&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"c10/util/Optional.h"</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>"c10/core/Stream.h"</cpp:file></cpp:include>

<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;stdexcept&gt;</cpp:file></cpp:include>

<cpp:ifndef>#<cpp:directive>ifndef</cpp:directive> <name>AT_PER_OPERATOR_HEADERS</name></cpp:ifndef>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;ATen/Functions.h&gt;</cpp:file></cpp:include>
<cpp:else>#<cpp:directive>else</cpp:directive></cpp:else>
<macro><name>$ops_headers</name></macro>
<cpp:endif>#<cpp:directive>endif</cpp:directive></cpp:endif>

<using>using <name><name>at</name><operator>::</operator><name>DeviceGuard</name></name>;</using>
<using>using <name><name>at</name><operator>::</operator><name>device_of</name></name>;</using>
<using>using <name><name>at</name><operator>::</operator><name>OptionalDeviceGuard</name></name>;</using>
<using>using <name><name>at</name><operator>::</operator><name>Backend</name></name>;</using>
<using>using <name><name>at</name><operator>::</operator><name>Scalar</name></name>;</using>
<using>using <name><name>at</name><operator>::</operator><name>ScalarType</name></name>;</using>
<using>using <name><name>at</name><operator>::</operator><name>Tensor</name></name>;</using>
<using>using <name><name>c10</name><operator>::</operator><name>Stream</name></name>;</using>
<using>using <namespace>namespace <name><name>torch</name><operator>::</operator><name>autograd</name><operator>::</operator><name>utils</name></name>;</namespace></using>

<namespace>namespace <name>torch</name> <block>{ <namespace>namespace <name>autograd</name> <block>{

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable__is_view</name><parameter_list>(<parameter><decl><type><name>PyObject</name> <modifier>*</modifier></type><name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><literal type="string">"_is_view"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <if_stmt><if>if <condition>(<expr><call><name><name>self_</name><operator>.</operator><name>is_view</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <expr_stmt><expr><name>Py_RETURN_TRUE</name></expr>;</expr_stmt>
  </block_content>}</block></if> <else>else <block>{<block_content>
    <expr_stmt><expr><name>Py_RETURN_FALSE</name></expr>;</expr_stmt>
  </block_content>}</block></else></if_stmt>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<comment type="line">// implemented on the python object bc no support for first-class functions in native_functions.yaml</comment>
<comment type="line">// See: ATen/native/README.md for more context</comment>
<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_apply_</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>arg</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <decl_stmt><decl><type><name>auto</name></type> <name>args</name> <init>= <expr><call><name><name>py</name><operator>::</operator><name>make_tuple</name></name><argument_list>(<argument><expr><call><name><name>py</name><operator>::</operator><name>handle</name></name><argument_list>(<argument><expr><name>arg</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><literal type="string">"apply_"</literal></expr></argument>, <argument><expr><call><name><name>args</name><operator>.</operator><name>ptr</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <if_stmt><if>if <condition>(<expr><call><name><name>self_</name><operator>.</operator><name>requires_grad</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name><name>std</name><operator>::</operator><name>runtime_error</name></name><argument_list>(
        <argument><expr><literal type="string">"Can't call apply_() on Variable that requires grad. Use "</literal>
        <literal type="string">"var.detach().apply_() instead."</literal></expr></argument>)</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
  <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name><name>torch</name><operator>::</operator><name>utils</name><operator>::</operator><name>apply_</name></name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><name>arg</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_size</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"size(int64_t dim)"</literal></expr>,
    <expr><literal type="string">"size()"</literal></expr>,
    <expr><literal type="string">"size(Dimname dim)"</literal></expr>,
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">3</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <if_stmt><if>if <condition>(<expr><name><name>r</name><operator>.</operator><name>idx</name></name> <operator>==</operator> <literal type="number">0</literal></expr>)</condition> <block>{<block_content>
    <if_stmt><if>if <condition>(<expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>isTracing</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
      <comment type="line">// will error out if a tensor has symints</comment>
      <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>getSizeOf</name></name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><call><name><name>r</name><operator>.</operator><name>toInt64</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
    </block_content>}</block></if> <else>else <block>{<block_content>
      <return>return <expr><call><name><name>torch</name><operator>::</operator><name>toPyObject</name></name><argument_list>(<argument><expr><call><name><name>self_</name><operator>.</operator><name>sym_size</name></name><argument_list>(<argument><expr><call><name><name>r</name><operator>.</operator><name>toInt64</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
    </block_content>}</block></else></if_stmt>
  </block_content>}</block></if> <if type="elseif">else if <condition>(<expr><name><name>r</name><operator>.</operator><name>idx</name></name> <operator>==</operator> <literal type="number">1</literal></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>THPSize_NewFromSymSizes</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if>
  <if type="elseif">else if <condition>(<expr><name><name>r</name><operator>.</operator><name>idx</name></name> <operator>==</operator> <literal type="number">2</literal></expr>)</condition> <block>{<block_content>
    <if_stmt><if>if <condition>(<expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>isTracing</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
      <expr_stmt><expr><call><name>TORCH_INTERNAL_ASSERT</name><argument_list>(<argument><expr><literal type="boolean">false</literal></expr></argument>, <argument><expr><literal type="string">"NYI: Named tensors w/ JIT"</literal></expr></argument>)</argument_list></call></expr>;</expr_stmt>
    </block_content>}</block></if></if_stmt>
    <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name><name>self_</name><operator>.</operator><name>size</name></name><argument_list>(<argument><expr><call><name><name>r</name><operator>.</operator><name>dimname</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><name>Py_RETURN_NONE</name></expr>;</expr_stmt>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_stride</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"stride(int64_t dim)"</literal></expr>,
    <expr><literal type="string">"stride()"</literal></expr>,
    <expr><literal type="string">"stride(Dimname dim)"</literal></expr>,
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">3</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <if_stmt><if>if <condition>(<expr><name><name>r</name><operator>.</operator><name>idx</name></name> <operator>==</operator> <literal type="number">0</literal></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name><name>torch</name><operator>::</operator><name>toPyObject</name></name><argument_list>(<argument><expr><call><name><name>self_</name><operator>.</operator><name>sym_stride</name></name><argument_list>(<argument><expr><call><name><name>r</name><operator>.</operator><name>toInt64</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if> <if type="elseif">else if <condition>(<expr><name><name>r</name><operator>.</operator><name>idx</name></name> <operator>==</operator> <literal type="number">1</literal></expr>)</condition> <block>{<block_content>
    <comment type="line">// yes, this is called strides in ATen.</comment>
    <decl_stmt><decl><type><name><name>at</name><operator>::</operator><name>SymIntArrayRef</name></name></type> <name>strides</name> <init>= <expr><call><name><name>self_</name><operator>.</operator><name>sym_strides</name></name><argument_list>()</argument_list></call></expr></init></decl>;</decl_stmt>
    <comment type="line">// we can't do the normal wrapping here because IntArrayRef maps to both</comment>
    <comment type="line">// torch.Size and tuple in python</comment>
    <comment type="line">// TODO: consider factoring this out</comment>
    <decl_stmt><decl><type><name>THPObjectPtr</name></type> <name>tuple</name><argument_list>(<argument><expr><call><name>PyTuple_New</name><argument_list>(<argument><expr><call><name><name>strides</name><operator>.</operator><name>size</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></decl>;</decl_stmt>
    <if_stmt><if>if <condition>(<expr><operator>!</operator><name>tuple</name></expr>)</condition><block type="pseudo"><block_content> <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw></block_content></block></if></if_stmt>
    <for>for <control>(<init><decl><type><name>size_t</name></type> <name>i</name> <init>= <expr><literal type="number">0</literal></expr></init></decl>;</init> <condition><expr><name>i</name> <operator>!=</operator> <call><name><name>strides</name><operator>.</operator><name>size</name></name><argument_list>()</argument_list></call></expr>;</condition> <incr><expr><name>i</name><operator>++</operator></expr></incr>)</control> <block>{<block_content>
      <decl_stmt><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>s</name> <init>= <expr><call><name><name>torch</name><operator>::</operator><name>toPyObject</name></name><argument_list>(<argument><expr><name><name>strides</name><index>[<expr><name>i</name></expr>]</index></name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
      <if_stmt><if>if <condition>(<expr><operator>!</operator><name>s</name></expr>)</condition><block type="pseudo"><block_content> <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw></block_content></block></if></if_stmt>
      <expr_stmt><expr><call><name>PyTuple_SET_ITEM</name><argument_list>(<argument><expr><call><name><name>tuple</name><operator>.</operator><name>get</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><name>i</name></expr></argument>, <argument><expr><name>s</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
    </block_content>}</block></for>
    <return>return <expr><call><name><name>tuple</name><operator>.</operator><name>release</name></name><argument_list>()</argument_list></call></expr>;</return>
  </block_content>}</block></if>
  <if type="elseif">else if <condition>(<expr><name><name>r</name><operator>.</operator><name>idx</name></name> <operator>==</operator> <literal type="number">2</literal></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name><name>self_</name><operator>.</operator><name>stride</name></name><argument_list>(<argument><expr><call><name><name>r</name><operator>.</operator><name>dimname</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><name>Py_RETURN_NONE</name></expr>;</expr_stmt>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<comment type="line">// implemented on the python object to avoid dispatch overhead</comment>
<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_get_device</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self_</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><literal type="string">"get_device"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><literal type="null">nullptr</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name><name>self</name><operator>.</operator><name>get_device</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_has_names</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self_</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><literal type="string">"has_names"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name><name>self</name><operator>.</operator><name>has_names</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<comment type="line">// implemented on the python object to avoid dispatch overhead</comment>
<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_data_ptr</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self_</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><literal type="string">"data_ptr"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name><name>self</name><operator>.</operator><name>data_ptr</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<comment type="line">// implemented on the python object to avoid dispatch overhead</comment>
<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_storage_offset</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self_</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><literal type="string">"storage_offset"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <return>return <expr><call><name><name>py</name><operator>::</operator><name>cast</name></name><argument_list>(<argument><expr><call><name><name>self</name><operator>.</operator><name>sym_storage_offset</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call><operator>.</operator><call><name>release</name><argument_list>()</argument_list></call><operator>.</operator><call><name>ptr</name><argument_list>()</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<comment type="line">// implemented on the python object to avoid dispatch overhead</comment>
<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_dim</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>)</parameter_list>
<block>{<block_content>
   <macro><name>HANDLE_TH_ERRORS</name></macro>
   <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
     <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><literal type="string">"dim"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>)</argument_list></call></expr>;</return>
   </block_content>}</block></if></if_stmt>
   <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
   <return>return <expr><call><name>THPUtils_packInt64</name><argument_list>(<argument><expr><call><name><name>self_</name><operator>.</operator><name>dim</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
   <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<comment type="line">// implemented on the python object to avoid dispatch overhead</comment>
<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_numel</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>)</parameter_list>
<block>{<block_content>
   <macro><name>HANDLE_TH_ERRORS</name></macro>
   <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
     <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><literal type="string">"numel"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>)</argument_list></call></expr>;</return>
   </block_content>}</block></if></if_stmt>
   <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
   <if_stmt><if>if <condition>(<expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>isTracing</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
     <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>getNumelOf</name></name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
   </block_content>}</block></if> <else>else <block>{<block_content>
     <return>return <expr><call><name><name>py</name><operator>::</operator><name>cast</name></name><argument_list>(<argument><expr><call><name><name>self_</name><operator>.</operator><name>sym_numel</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call><operator>.</operator><call><name>release</name><argument_list>()</argument_list></call><operator>.</operator><call><name>ptr</name><argument_list>()</argument_list></call></expr>;</return>
   </block_content>}</block></else></if_stmt>
   <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>Tensor</name></type> <name>dispatch_contiguous</name><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name> <modifier>&amp;</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name><name>at</name><operator>::</operator><name>MemoryFormat</name></name></type> <name>memory_format</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>OptionalDeviceGuard</name></type> <name>device_guard</name><argument_list>(<argument><expr><call><name>device_of</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></decl>;</decl_stmt>
  <return>return <expr><call><name><name>self</name><operator>.</operator><name>contiguous</name></name><argument_list>(<argument><expr><name>memory_format</name></expr></argument>)</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_contiguous</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"contiguous(*, MemoryFormat memory_format=contiguous_format)"</literal></expr>,
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformat</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <comment type="line">// avoids touching the GIL or current device if self is already contiguous</comment>
  <if_stmt><if>if <condition>(<expr><call><name><name>self_</name><operator>.</operator><name>is_contiguous</name></name><argument_list>(<argument><expr><name>memory_format</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <comment type="line">// NOTE: this logic is duplicated from VariableType.cpp. Since we need to</comment>
    <comment type="line">// record this call to contiguous() in the trace regardless of whether</comment>
    <comment type="line">// we actually call contiguous here, we need to record this information</comment>
    <comment type="line">// manually.</comment>
    <if_stmt><if>if <condition>(<expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>isTracing</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
      <decl_stmt><decl><type><name>auto</name></type> <name>tracer_state</name> <init>= <expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>getTracingState</name></name><argument_list>()</argument_list></call></expr></init></decl>;</decl_stmt>
      <decl_stmt><decl><type><name>auto</name></type> <name>op_name</name> <init>= <expr><call><name><name>c10</name><operator>::</operator><name>Symbol</name><operator>::</operator><name>fromQualString</name></name><argument_list>(<argument><expr><literal type="string">"aten::contiguous"</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
      <decl_stmt><decl><type><name>auto</name></type> <name>node</name> <init>= <expr><call><name><name>tracer_state</name><operator>-&gt;</operator><name>createNode</name></name><argument_list>(<argument><expr><name>op_name</name></expr></argument>, <comment type="block">/*num_outputs=*/</comment><argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
      <expr_stmt><expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>recordSourceLocation</name></name><argument_list>(<argument><expr><name>node</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
      <expr_stmt><expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>addInputs</name></name><argument_list>(<argument><expr><name>node</name></expr></argument>, <argument><expr><literal type="string">"self"</literal></expr></argument>, <argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
      <expr_stmt><expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>addInputs</name></name><argument_list>(<argument><expr><name>node</name></expr></argument>, <argument><expr><literal type="string">"memory_format"</literal></expr></argument>, <argument><expr><name>memory_format</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
      <expr_stmt><expr><call><name><name>tracer_state</name><operator>-&gt;</operator><name>insertNode</name></name><argument_list>(<argument><expr><name>node</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
      <expr_stmt><expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>addOutput</name></name><argument_list>(<argument><expr><name>node</name></expr></argument>, <argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
    </block_content>}</block></if></if_stmt>
    <expr_stmt><expr><call><name>Py_INCREF</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
    <return>return <expr><name>self</name></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name>dispatch_contiguous</name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><name>memory_format</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>Tensor</name></type> <name>dispatch_copy_</name><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name> <modifier>&amp;</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><specifier>const</specifier> <name>Tensor</name> <modifier>&amp;</modifier></type> <name>other</name></decl></parameter>, <parameter><decl><type><name>bool</name></type> <name>non_blocking</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>OptionalDeviceGuard</name></type> <name>device_guard</name><argument_list>(<argument><expr><call><name>device_of</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></decl>;</decl_stmt>
  <return>return <expr><call><name><name>self</name><operator>.</operator><name>copy_</name></name><argument_list>(<argument><expr><name>other</name></expr></argument>, <argument><expr><name>non_blocking</name></expr></argument>)</argument_list></call></expr>;</return>
</block_content>}</block></function>

 <function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_copy_</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"copy_(Tensor other, bool non_blocking=False)"</literal></expr>,
    <expr><literal type="string">"copy_(Tensor other, bool async=False)|deprecated"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">2</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name>dispatch_copy_</name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><call><name><name>r</name><operator>.</operator><name>tensor</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></argument>, <argument><expr><call><name><name>r</name><operator>.</operator><name>toBool</name></name><argument_list>(<argument><expr><literal type="number">1</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>double</name></type> <name>dispatch_to_CDouble</name><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name> <modifier>&amp;</modifier></type> <name>self</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>OptionalDeviceGuard</name></type> <name>device_guard</name><argument_list>(<argument><expr><call><name>device_of</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><call><name><name>self</name><operator>.</operator><name>numel</name></name><argument_list>()</argument_list></call> <operator>!=</operator> <literal type="number">1</literal></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name>ValueError</name><argument_list>(<argument><expr><literal type="string">"only one element tensors can be converted to Python scalars"</literal></expr></argument>)</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
  <return>return <expr><call><name><name>self</name><operator>.</operator><name>item</name><argument_list type="generic">&lt;<argument><expr><name>double</name></expr></argument>&gt;</argument_list></name><argument_list>()</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name><name>c10</name><operator>::</operator><name>complex</name><argument_list type="generic">&lt;<argument><expr><name>double</name></expr></argument>&gt;</argument_list></name></type> <name>dispatch_to_CComplexDouble</name><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name> <modifier>&amp;</modifier></type> <name>self</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>OptionalDeviceGuard</name></type> <name>device_guard</name><argument_list>(<argument><expr><call><name>device_of</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><call><name><name>self</name><operator>.</operator><name>numel</name></name><argument_list>()</argument_list></call> <operator>!=</operator> <literal type="number">1</literal></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name>ValueError</name><argument_list>(<argument><expr><literal type="string">"only one element tensors can be converted to Python scalars"</literal></expr></argument>)</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
  <return>return <expr><call><name><name>self</name><operator>.</operator><name>item</name><argument_list type="generic">&lt;<argument><expr><name><name>c10</name><operator>::</operator><name>complex</name><argument_list type="generic">&lt;<argument><expr><name>double</name></expr></argument>&gt;</argument_list></name></expr></argument>&gt;</argument_list></name><argument_list>()</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>int64_t</name></type> <name>dispatch_to_CLong</name><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name> <modifier>&amp;</modifier></type> <name>self</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>OptionalDeviceGuard</name></type> <name>device_guard</name><argument_list>(<argument><expr><call><name>device_of</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><call><name><name>self</name><operator>.</operator><name>numel</name></name><argument_list>()</argument_list></call> <operator>!=</operator> <literal type="number">1</literal></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name>ValueError</name><argument_list>(<argument><expr><literal type="string">"only one element tensors can be converted to Python scalars"</literal></expr></argument>)</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
  <return>return <expr><call><name><name>self</name><operator>.</operator><name>item</name><argument_list type="generic">&lt;<argument><expr><name>int64_t</name></expr></argument>&gt;</argument_list></name><argument_list>()</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>bool</name></type> <name>dispatch_to_Bool</name><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name> <modifier>&amp;</modifier></type> <name>self</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>OptionalDeviceGuard</name></type> <name>device_guard</name><argument_list>(<argument><expr><call><name>device_of</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><call><name><name>self</name><operator>.</operator><name>numel</name></name><argument_list>()</argument_list></call> <operator>!=</operator> <literal type="number">1</literal></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name>ValueError</name><argument_list>(<argument><expr><literal type="string">"only one element tensors can be converted to Python scalars"</literal></expr></argument>)</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
  <return>return <expr><call><name><name>self</name><operator>.</operator><name>item</name><argument_list type="generic">&lt;<argument><expr><name>bool</name></expr></argument>&gt;</argument_list></name><argument_list>()</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_float_scalar</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><literal type="string">"__float__"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>warn</name></name><argument_list>(<argument><expr><literal type="string">"Converting a tensor to a Python float"</literal></expr></argument>, <argument><expr><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>WARN_PYTHON_DATAFLOW</name></name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name>dispatch_to_CDouble</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_complex_scalar</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><literal type="string">"__complex__"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>warn</name></name><argument_list>(<argument><expr><literal type="string">"Converting a tensor to a Python complex"</literal></expr></argument>, <argument><expr><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>WARN_PYTHON_DATAFLOW</name></name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name>dispatch_to_CComplexDouble</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_integral_scalar</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><literal type="string">"__int__"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>warn</name></name><argument_list>(<argument><expr><literal type="string">"Converting a tensor to a Python integer"</literal></expr></argument>, <argument><expr><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>WARN_PYTHON_DATAFLOW</name></name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <if_stmt><if>if <condition>(<expr><call><name>isFloatingType</name><argument_list>(<argument><expr><call><name><name>self_</name><operator>.</operator><name>scalar_type</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <comment type="line">// we can't dispatch to item&lt;int64_t&gt; here because we want to avoid ATen overflow checks;</comment>
    <comment type="line">// the python integral type (long in python2) can't overflow.</comment>
    <return>return <expr><call><name>THPUtils_packDoubleAsInt</name><argument_list>(<argument><expr><call><name>dispatch_to_CDouble</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if> <else>else <block>{<block_content>
    <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name>dispatch_to_CLong</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></else></if_stmt>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<comment type="line">// This is the __index__ function in Python which is similar to __int__, but</comment>
<comment type="line">// called when used as a slice.</comment>
<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_index_scalar</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><literal type="string">"__index__"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <comment type="line">// TODO: change the condition to `self_.dim() != 0` once we expose scalars</comment>
  <comment type="line">// in PyTorch.</comment>
  <if_stmt><if>if <condition>(<expr><operator>!</operator><call><name>isIntegralType</name><argument_list>(<argument><expr><call><name><name>self_</name><operator>.</operator><name>scalar_type</name></name><argument_list>()</argument_list></call></expr></argument>, <comment type="block">/*includeBool=*/</comment><argument><expr><literal type="boolean">true</literal></expr></argument>)</argument_list></call> <operator>||</operator> <call><name><name>self_</name><operator>.</operator><name>numel</name></name><argument_list>()</argument_list></call> <operator>!=</operator> <literal type="number">1</literal></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name>TypeError</name><argument_list>(<argument><expr><literal type="string">"only integer tensors of a single element can be converted to an index"</literal></expr></argument>)</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
  <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name>dispatch_to_CLong</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>Tensor</name></type> <name>dispatch_invert</name><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name> <modifier>&amp;</modifier></type> <name>self</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>OptionalDeviceGuard</name></type> <name>device_guard</name><argument_list>(<argument><expr><call><name>device_of</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></decl>;</decl_stmt>
  <return>return <expr><call><name><name>self</name><operator>.</operator><name>bitwise_not</name></name><argument_list>()</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_invert</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><literal type="string">"__invert__"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <if_stmt><if>if <condition>(<expr><operator>!</operator><call><name>isIntegralType</name><argument_list>(<argument><expr><call><name><name>self_</name><operator>.</operator><name>scalar_type</name></name><argument_list>()</argument_list></call></expr></argument>, <comment type="block">/*includeBool=*/</comment><argument><expr><literal type="boolean">true</literal></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name>TypeError</name><argument_list>(<argument><expr><literal type="string">"~ (operator.invert) is only implemented on integer and Boolean-type tensors"</literal></expr></argument>)</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
  <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name>dispatch_invert</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>Tensor</name></type> <name>dispatch_to</name><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name> <modifier>&amp;</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>Device</name></type> <name>device</name></decl></parameter>, <parameter><decl><type><name>bool</name></type> <name>non_blocking</name></decl></parameter>, <parameter><decl><type><name>bool</name></type> <name>copy</name></decl></parameter>, <parameter><decl><type><name><name>c10</name><operator>::</operator><name>optional</name><argument_list type="generic">&lt;<argument><expr><name><name>c10</name><operator>::</operator><name>MemoryFormat</name></name></expr></argument>&gt;</argument_list></name></type> <name>optional_memory_format</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
  <comment type="line">// NOTE: this is where we record aten::to in the graph during tracing. However, the behavior of aten::to</comment>
  <comment type="line">// is different with respect to TensorOptions fields that are not present: aten::to inherits fields that</comment>
  <comment type="line">// are missing from the self argument while the tracer assumes that they should be populated with the</comment>
  <comment type="line">// default values (eg. float for scalar type). By explicitly copying over the tensor options here we fully</comment>
  <comment type="line">// specify all tensor options and thus record the proper trace</comment>
  <return>return <expr><call><name><name>self</name><operator>.</operator><name>to</name></name><argument_list>(<argument><expr><call><name><name>self</name><operator>.</operator><name>options</name></name><argument_list>()</argument_list></call><operator>.</operator><call><name>device</name><argument_list>(<argument><expr><name>device</name></expr></argument>)</argument_list></call><operator>.</operator><call><name>memory_format</name><argument_list>(<argument><expr><name>optional_memory_format</name></expr></argument>)</argument_list></call></expr></argument>, <argument><expr><name>non_blocking</name></expr></argument>, <argument><expr><name>copy</name></expr></argument>)</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>Tensor</name></type> <name>dispatch_to</name><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name> <modifier>&amp;</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>bool</name></type> <name>non_blocking</name></decl></parameter>, <parameter><decl><type><name>bool</name></type> <name>copy</name></decl></parameter>, <parameter><decl><type><name><name>c10</name><operator>::</operator><name>optional</name><argument_list type="generic">&lt;<argument><expr><name><name>c10</name><operator>::</operator><name>MemoryFormat</name></name></expr></argument>&gt;</argument_list></name></type> <name>optional_memory_format</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>AutoNoGIL</name></type> <name>no_gil</name></decl>;</decl_stmt>
  <return>return <expr><call><name><name>self</name><operator>.</operator><name>to</name></name><argument_list>(<argument><expr><call><name><name>self</name><operator>.</operator><name>options</name></name><argument_list>()</argument_list></call><operator>.</operator><call><name>memory_format</name><argument_list>(<argument><expr><name>optional_memory_format</name></expr></argument>)</argument_list></call></expr></argument>, <argument><expr><name>non_blocking</name></expr></argument>, <argument><expr><name>copy</name></expr></argument>)</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>Tensor</name></type> <name>dispatch_to</name><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name> <modifier>&amp;</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>ScalarType</name></type> <name>dtype</name></decl></parameter>, <parameter><decl><type><name>bool</name></type> <name>non_blocking</name></decl></parameter>, <parameter><decl><type><name>bool</name></type> <name>copy</name></decl></parameter>, <parameter><decl><type><name><name>c10</name><operator>::</operator><name>optional</name><argument_list type="generic">&lt;<argument><expr><name><name>c10</name><operator>::</operator><name>MemoryFormat</name></name></expr></argument>&gt;</argument_list></name></type> <name>optional_memory_format</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
  <comment type="line">// TODO: Make this call the TensorOptions version, maybe?</comment>
  <return>return <expr><call><name><name>self</name><operator>.</operator><name>to</name></name><argument_list>(<argument><expr><name>dtype</name></expr></argument>, <argument><expr><name>non_blocking</name></expr></argument>, <argument><expr><name>copy</name></expr></argument>, <argument><expr><name>optional_memory_format</name></expr></argument>)</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>Tensor</name></type> <name>dispatch_to</name><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name> <modifier>&amp;</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>Device</name></type> <name>device</name></decl></parameter>, <parameter><decl><type><name>ScalarType</name></type> <name>dtype</name></decl></parameter>, <parameter><decl><type><name>bool</name></type> <name>non_blocking</name></decl></parameter>, <parameter><decl><type><name>bool</name></type> <name>copy</name></decl></parameter>, <parameter><decl><type><name><name>c10</name><operator>::</operator><name>optional</name><argument_list type="generic">&lt;<argument><expr><name><name>c10</name><operator>::</operator><name>MemoryFormat</name></name></expr></argument>&gt;</argument_list></name></type> <name>optional_memory_format</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
  <comment type="line">// TODO: Make this call the TensorOptions version, maybe?</comment>
  <return>return <expr><call><name><name>self</name><operator>.</operator><name>to</name></name><argument_list>(<argument><expr><name>device</name></expr></argument>, <argument><expr><name>dtype</name></expr></argument>, <argument><expr><name>non_blocking</name></expr></argument>, <argument><expr><name>copy</name></expr></argument>, <argument><expr><name>optional_memory_format</name></expr></argument>)</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_cpu</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
   <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
   <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
     <expr><literal type="string">"cpu(*, MemoryFormat? memory_format=None)"</literal></expr>
   }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
   <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
   <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
   <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

   <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
    </block_content>}</block></if></if_stmt>

   <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
   <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name>dispatch_to</name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><call><name><name>at</name><operator>::</operator><name>Device</name></name><argument_list>(<argument><expr><name><name>at</name><operator>::</operator><name>DeviceType</name><operator>::</operator><name>CPU</name></name></expr></argument>)</argument_list></call></expr></argument>, <argument><expr><literal type="boolean">false</literal></expr></argument>, <argument><expr><literal type="boolean">false</literal></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
   <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>Tensor</name></type> <name>dispatch_nonzero</name><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name> <modifier>&amp;</modifier></type> <name>self</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>OptionalDeviceGuard</name></type> <name>device_guard</name><argument_list>(<argument><expr><call><name>device_of</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></decl>;</decl_stmt>
  <return>return <expr><call><name><name>self</name><operator>.</operator><name>nonzero</name></name><argument_list>()</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name><name>std</name><operator>::</operator><name>vector</name><argument_list type="generic">&lt;<argument><expr><name>Tensor</name></expr></argument>&gt;</argument_list></name></type> <name>dispatch_nonzero_numpy</name><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name> <modifier>&amp;</modifier></type> <name>self</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>OptionalDeviceGuard</name></type> <name>device_guard</name><argument_list>(<argument><expr><call><name>device_of</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></decl>;</decl_stmt>
  <return>return <expr><call><name><name>self</name><operator>.</operator><name>nonzero_numpy</name></name><argument_list>()</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_nonzero</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"nonzero()"</literal></expr>,
    <expr><literal type="string">"nonzero(*, bool as_tuple)"</literal></expr>,
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">2</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <if_stmt><if>if <condition>(<expr><name><name>r</name><operator>.</operator><name>idx</name></name> <operator>==</operator> <literal type="number">0</literal> <operator>||</operator> <operator>(</operator><name><name>r</name><operator>.</operator><name>idx</name></name> <operator>==</operator> <literal type="number">1</literal> <operator>&amp;&amp;</operator> <operator>!</operator><call><name><name>r</name><operator>.</operator><name>toBool</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call><operator>)</operator></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name>dispatch_nonzero</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if> <else>else <block>{<block_content>
    <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name>dispatch_nonzero_numpy</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></else></if_stmt>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_cuda</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"cuda(Device? device=None, bool non_blocking=False, *, MemoryFormat? memory_format=None)"</literal></expr>,
    <expr><literal type="string">"cuda(Device? device=None, bool async=False, *, MemoryFormat? memory_format=None)|deprecated"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">3</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>device</name> <init>= <expr><ternary><condition><expr><call><name><name>r</name><operator>.</operator><name>isNone</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr> ?</condition><then> <expr><call><name><name>at</name><operator>::</operator><name>Device</name></name><argument_list>(<argument><expr><name><name>at</name><operator>::</operator><name>DeviceType</name><operator>::</operator><name>CUDA</name></name></expr></argument>)</argument_list></call></expr> </then><else>: <expr><call><name><name>r</name><operator>.</operator><name>device</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></else></ternary></expr></init></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">2</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <expr_stmt><expr><call><name>TORCH_CHECK</name><argument_list>(<argument><expr><call><name><name>device</name><operator>.</operator><name>is_cuda</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><literal type="string">"Invalid device, must be cuda device"</literal></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <expr_stmt><expr><call><name><name>torch</name><operator>::</operator><name>utils</name><operator>::</operator><name>cuda_lazy_init</name></name><argument_list>()</argument_list></call></expr>;</expr_stmt>
  <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name>dispatch_to</name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><name>device</name></expr></argument>, <argument><expr><call><name><name>r</name><operator>.</operator><name>toBool</name></name><argument_list>(<argument><expr><literal type="number">1</literal></expr></argument>)</argument_list></call></expr></argument>, <argument><expr><literal type="boolean">false</literal></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_xpu</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"xpu(Device? device=None, bool non_blocking=False, *, MemoryFormat? memory_format=None)"</literal></expr>,
    <expr><literal type="string">"xpu(Device? device=None, bool async=False, *, MemoryFormat? memory_format=None)|deprecated"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">3</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if <condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>device</name> <init>= <expr><ternary><condition><expr><call><name><name>r</name><operator>.</operator><name>isNone</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr> ?</condition><then> <expr><call><name><name>at</name><operator>::</operator><name>Device</name></name><argument_list>(<argument><expr><name><name>at</name><operator>::</operator><name>DeviceType</name><operator>::</operator><name>XPU</name></name></expr></argument>)</argument_list></call></expr> </then><else>: <expr><call><name><name>r</name><operator>.</operator><name>device</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></else></ternary></expr></init></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">2</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <expr_stmt><expr><call><name>TORCH_CHECK</name><argument_list>(<argument><expr><call><name><name>device</name><operator>.</operator><name>is_xpu</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><literal type="string">"Invalid device, must be xpu device"</literal></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name>dispatch_to</name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><name>device</name></expr></argument>, <argument><expr><call><name><name>r</name><operator>.</operator><name>toBool</name></name><argument_list>(<argument><expr><literal type="number">1</literal></expr></argument>)</argument_list></call></expr></argument>, <argument><expr><literal type="boolean">false</literal></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_ipu</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"ipu(Device? device=None, bool non_blocking=False, *, MemoryFormat? memory_format=None)"</literal></expr>,
    <expr><literal type="string">"ipu(Device? device=None, bool async=False, *, MemoryFormat? memory_format=None)|deprecated"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">3</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if <condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>device</name> <init>= <expr><ternary><condition><expr><call><name><name>r</name><operator>.</operator><name>isNone</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr> ?</condition><then> <expr><call><name><name>at</name><operator>::</operator><name>Device</name></name><argument_list>(<argument><expr><name><name>at</name><operator>::</operator><name>DeviceType</name><operator>::</operator><name>IPU</name></name></expr></argument>)</argument_list></call></expr> </then><else>: <expr><call><name><name>r</name><operator>.</operator><name>device</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></else></ternary></expr></init></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">2</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <expr_stmt><expr><call><name>TORCH_CHECK</name><argument_list>(<argument><expr><call><name><name>device</name><operator>.</operator><name>is_ipu</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><literal type="string">"Invalid device, must be ipu device"</literal></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name>dispatch_to</name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><name>device</name></expr></argument>, <argument><expr><call><name><name>r</name><operator>.</operator><name>toBool</name></name><argument_list>(<argument><expr><literal type="number">1</literal></expr></argument>)</argument_list></call></expr></argument>, <argument><expr><literal type="boolean">false</literal></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_to_type</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>ScalarType</name></type> <name>scalarType</name></decl></parameter>, <parameter><decl><type><name><name>c10</name><operator>::</operator><name>optional</name><argument_list type="generic">&lt;<argument><expr><name><name>c10</name><operator>::</operator><name>MemoryFormat</name></name></expr></argument>&gt;</argument_list></name></type> <name>optional_memory_format</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>auto</specifier><modifier>&amp;</modifier></type> <name>self_</name> <init>= <expr><call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name>dispatch_to</name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><name>scalarType</name></expr></argument>, <argument><expr><literal type="boolean">false</literal></expr></argument>, <argument><expr><literal type="boolean">false</literal></expr></argument>, <argument><expr><name>optional_memory_format</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_byte</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>  <block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"byte(*, MemoryFormat? memory_format=None)"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <return>return <expr><call><name>THPVariable_to_type</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name><name>ScalarType</name><operator>::</operator><name>Byte</name></name></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_char</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>  <block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"char(*, MemoryFormat? memory_format=None)"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <return>return <expr><call><name>THPVariable_to_type</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name><name>ScalarType</name><operator>::</operator><name>Char</name></name></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_double</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"double(*, MemoryFormat? memory_format=None)"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <return>return <expr><call><name>THPVariable_to_type</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name><name>ScalarType</name><operator>::</operator><name>Double</name></name></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_float</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"float(*, MemoryFormat? memory_format=None)"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <return>return <expr><call><name>THPVariable_to_type</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name><name>ScalarType</name><operator>::</operator><name>Float</name></name></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_cdouble</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"cdouble(*, MemoryFormat? memory_format=None)"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <return>return <expr><call><name>THPVariable_to_type</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name><name>ScalarType</name><operator>::</operator><name>ComplexDouble</name></name></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_cfloat</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"cfloat(*, MemoryFormat? memory_format=None)"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <return>return <expr><call><name>THPVariable_to_type</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name><name>ScalarType</name><operator>::</operator><name>ComplexFloat</name></name></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_half</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"half(*, MemoryFormat? memory_format=None)"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <return>return <expr><call><name>THPVariable_to_type</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name><name>ScalarType</name><operator>::</operator><name>Half</name></name></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_int</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"int(*, MemoryFormat? memory_format=None)"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <return>return <expr><call><name>THPVariable_to_type</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name><name>ScalarType</name><operator>::</operator><name>Int</name></name></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_long</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"long(*, MemoryFormat? memory_format=None)"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <return>return <expr><call><name>THPVariable_to_type</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name><name>ScalarType</name><operator>::</operator><name>Long</name></name></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_short</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"short(*, MemoryFormat? memory_format=None)"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <return>return <expr><call><name>THPVariable_to_type</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name><name>ScalarType</name><operator>::</operator><name>Short</name></name></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_bool</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"bool(*, MemoryFormat? memory_format=None)"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <return>return <expr><call><name>THPVariable_to_type</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name><name>ScalarType</name><operator>::</operator><name>Bool</name></name></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_bfloat16</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"bfloat16(*, MemoryFormat? memory_format=None)"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <return>return <expr><call><name>THPVariable_to_type</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name><name>ScalarType</name><operator>::</operator><name>BFloat16</name></name></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_element_size</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><literal type="string">"element_size"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <return>return <expr><call><name>THPUtils_packInt64</name><argument_list>(<argument><expr><call><name><name>self_</name><operator>.</operator><name>element_size</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<comment type="line">// implemented on the python object bc PyObjects not declarable in native_functions.yaml</comment>
<comment type="line">// See: ATen/native/README.md for more context</comment>
<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_numpy</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"numpy(*, bool force=False)"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if <condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <expr_stmt><expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>warn</name></name><argument_list>(<argument><expr><literal type="string">"Converting a tensor to a NumPy array"</literal></expr></argument>, <argument><expr><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>WARN_PYTHON_DATAFLOW</name></name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <return>return <expr><call><name><name>torch</name><operator>::</operator><name>utils</name><operator>::</operator><name>tensor_to_numpy</name></name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><call><name><name>r</name><operator>.</operator><name>toBool</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_requires_grad_</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"requires_grad_(bool requires_grad=True)"</literal></expr>,
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <comment type="line">// temporary hack to improve functorch UX.</comment>
  <decl_stmt><decl><type><specifier>const</specifier> <specifier>auto</specifier><modifier>&amp;</modifier></type> <name>functorch_tls</name> <init>= <expr><call><name><name>at</name><operator>::</operator><name>functorch</name><operator>::</operator><name>functorchTLSAccessor</name></name><argument_list>()</argument_list></call></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><name>functorch_tls</name></expr>)</condition> <block>{<block_content>
    <expr_stmt><expr><call><name><name>functorch_tls</name><operator>-&gt;</operator><name>checkSupportsInplaceRequiresGrad</name></name><argument_list>()</argument_list></call></expr>;</expr_stmt>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>requires_grad</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>toBool</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <comment type="line">// should we throw if requires_grad is true?  var.requires_grad = True throws here</comment>
  <comment type="line">// but it's nice to let this be a no-op.</comment>
  <if_stmt><if>if <condition>(<expr><operator>!</operator><call><name><name>self_</name><operator>.</operator><name>is_leaf</name></name><argument_list>()</argument_list></call> <operator>&amp;&amp;</operator> <operator>!</operator><name>requires_grad</name></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name><name>std</name><operator>::</operator><name>runtime_error</name></name><argument_list>(<argument><expr><call><name><name>autograd</name><operator>::</operator><name>utils</name><operator>::</operator><name>requires_grad_leaf_error</name></name><argument_list>(<argument><expr><name>requires_grad</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
  <if_stmt><if>if <condition>(<expr><name>requires_grad</name> <operator>&amp;&amp;</operator> <operator>!</operator> <call><name>isDifferentiableType</name><argument_list>(<argument><expr><call><name><name>at</name><operator>::</operator><name>typeMetaToScalarType</name></name><argument_list>(<argument><expr><call><name><name>self_</name><operator>.</operator><name>dtype</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name><name>std</name><operator>::</operator><name>runtime_error</name></name><argument_list>(<argument><expr><literal type="string">"only Tensors of floating point dtype can require gradients"</literal></expr></argument>)</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><call><name><name>self_</name><operator>.</operator><name>set_requires_grad</name></name><argument_list>(<argument><expr><name>requires_grad</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>inline</specifier> <name>bool</name></type> <name>dispatch_is_contiguous</name><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name> <modifier>&amp;</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>MemoryFormat</name></type> <name>memory_format</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <return>return <expr><call><name><name>self</name><operator>.</operator><name>is_contiguous</name></name><argument_list>(<argument><expr><name>memory_format</name></expr></argument>)</argument_list></call></expr>;</return>
</block_content>}</block></function>

<comment type="line">// implemented on the python object to avoid dispatch overhead</comment>
<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_is_contiguous</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self_</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"is_contiguous(*, MemoryFormat memory_format=contiguous_format)"</literal></expr>,
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self_</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><call><name>PyObject_Type</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformat</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name>dispatch_is_contiguous</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>memory_format</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<comment type="line">// implemented on the python object to avoid dispatch overhead</comment>
<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_item</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><literal type="string">"item"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>warn</name></name><argument_list>(<argument><expr><literal type="string">"Converting a tensor to a Python number"</literal></expr></argument>, <argument><expr><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>WARN_PYTHON_DATAFLOW</name></name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <if_stmt><if>if <condition>(<expr><call><name><name>self_</name><operator>.</operator><name>is_floating_point</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name>dispatch_to_CDouble</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if> <if type="elseif">else if <condition>(<expr><call><name><name>self_</name><operator>.</operator><name>is_complex</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name>dispatch_to_CComplexDouble</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if> <if type="elseif">else if <condition>(<expr><call><name><name>self_</name><operator>.</operator><name>scalar_type</name></name><argument_list>()</argument_list></call> <operator>==</operator> <name><name>ScalarType</name><operator>::</operator><name>Bool</name></name></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name>dispatch_to_Bool</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if> <else>else <block>{<block_content>
    <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name>dispatch_to_CLong</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></else></if_stmt>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<comment type="line">// implemented on the python object bc no support for first class functions in native_functions.yaml</comment>
<comment type="line">// See: ATen/native/README.md for more context</comment>
<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_map_</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{ <expr><literal type="string">"map_(Tensor other, PyObject* callable)"</literal></expr> }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">2</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>Variable</name></type> <name>other</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>tensor</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><call><name><name>self_</name><operator>.</operator><name>requires_grad</name></name><argument_list>()</argument_list></call> <operator>||</operator> <call><name><name>other</name><operator>.</operator><name>requires_grad</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name><name>std</name><operator>::</operator><name>runtime_error</name></name><argument_list>(
        <argument><expr><literal type="string">"Can't call map_() on Variable that requires grad. Use "</literal>
        <literal type="string">"var.detach().map_() instead."</literal></expr></argument>)</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><call><name>TORCH_CHECK</name><argument_list>(
      <argument><expr><operator>!</operator><call><name><name>self_</name><operator>.</operator><name>unsafeGetTensorImpl</name></name><argument_list>()</argument_list></call><operator>-&gt;</operator><call><name>is_python_dispatch</name><argument_list>()</argument_list></call> <operator>&amp;&amp;</operator> <operator>!</operator><call><name><name>other</name><operator>.</operator><name>unsafeGetTensorImpl</name></name><argument_list>()</argument_list></call><operator>-&gt;</operator><call><name>is_python_dispatch</name><argument_list>()</argument_list></call></expr></argument>,
      <argument><expr><literal type="string">".map_ is not supported for tensor subclasses."</literal></expr></argument>)</argument_list></call></expr>;</expr_stmt>

  <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name><name>torch</name><operator>::</operator><name>utils</name><operator>::</operator><name>map_</name></name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><name>other</name></expr></argument>, <argument><expr><call><name><name>r</name><operator>.</operator><name>pyobject</name></name><argument_list>(<argument><expr><literal type="number">1</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<comment type="line">// implemented on the python object bc no support for first class functions in native_functions.yaml</comment>
<comment type="line">// See: ATen/native/README.md for more context</comment>
<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_map2_</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{ <expr><literal type="string">"map2_(Tensor x, Tensor y, PyObject* callable)"</literal></expr> }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">3</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <decl_stmt><decl><type><name>Variable</name></type> <name>x</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>tensor</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>Variable</name></type> <name>y</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>tensor</name></name><argument_list>(<argument><expr><literal type="number">1</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><call><name><name>self_</name><operator>.</operator><name>requires_grad</name></name><argument_list>()</argument_list></call> <operator>||</operator> <call><name><name>x</name><operator>.</operator><name>requires_grad</name></name><argument_list>()</argument_list></call> <operator>||</operator> <call><name><name>y</name><operator>.</operator><name>requires_grad</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name><name>std</name><operator>::</operator><name>runtime_error</name></name><argument_list>(
        <argument><expr><literal type="string">"Can't call map2_() on Variable that requires grad. Use "</literal>
        <literal type="string">"var.detach().map2_() instead."</literal></expr></argument>)</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><call><name>TORCH_CHECK</name><argument_list>(
      <argument><expr><operator>!</operator><call><name><name>x</name><operator>.</operator><name>unsafeGetTensorImpl</name></name><argument_list>()</argument_list></call><operator>-&gt;</operator><call><name>is_python_dispatch</name><argument_list>()</argument_list></call> <operator>&amp;&amp;</operator> <operator>!</operator><call><name><name>y</name><operator>.</operator><name>unsafeGetTensorImpl</name></name><argument_list>()</argument_list></call><operator>-&gt;</operator><call><name>is_python_dispatch</name><argument_list>()</argument_list></call></expr></argument>,
      <argument><expr><literal type="string">".map2_ is not supported for tensor subclasses."</literal></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name><name>torch</name><operator>::</operator><name>utils</name><operator>::</operator><name>map2_</name></name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><name>x</name></expr></argument>, <argument><expr><name>y</name></expr></argument>, <argument><expr><call><name><name>r</name><operator>.</operator><name>pyobject</name></name><argument_list>(<argument><expr><literal type="number">2</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_new</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><literal type="string">"new"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name>OptionalDeviceGuard</name></type> <name>device_guard</name><argument_list>(<argument><expr><call><name>device_of</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></decl>;</decl_stmt>
  <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name><name>torch</name><operator>::</operator><name>utils</name><operator>::</operator><name>legacy_tensor_new</name></name><argument_list>(<argument><expr><call><name>legacyExtractDispatchKey</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>, <argument><expr><call><name><name>self_</name><operator>.</operator><name>scalar_type</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_new_tensor</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><literal type="string">"new_tensor"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name>OptionalDeviceGuard</name></type> <name>device_guard</name><argument_list>(<argument><expr><call><name>device_of</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></decl>;</decl_stmt>
  <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name><name>torch</name><operator>::</operator><name>utils</name><operator>::</operator><name>new_tensor</name></name><argument_list>(<argument><expr><call><name>legacyExtractDispatchKey</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></argument>, <argument><expr><call><name><name>self_</name><operator>.</operator><name>scalar_type</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_storage</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>arg</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><literal type="string">"_storage"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <return>return <expr><call><name>createPyObject</name><argument_list>(<argument><expr><call><name><name>self_</name><operator>.</operator><name>storage</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_to</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"to(Device device=None, ScalarType dtype=None, bool non_blocking=False, bool copy=False, *, MemoryFormat? memory_format=None)"</literal></expr>,
    <expr><literal type="string">"to(ScalarType dtype, bool non_blocking=False, bool copy=False, *, MemoryFormat? memory_format=None)"</literal></expr>,
    <expr><literal type="string">"to(Tensor tensor, bool non_blocking=False, bool copy=False, *, MemoryFormat? memory_format=None)"</literal></expr>,
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">5</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>parsed</name> <init>= <expr><call><name>parse_to_conversion</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <comment type="block">/*allow_copy*/</comment> <argument><expr><literal type="boolean">true</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>device</name> <operator>=</operator> <call><name><name>std</name><operator>::</operator><name>get</name><argument_list type="generic">&lt;<argument><expr><literal type="number">0</literal></expr></argument>&gt;</argument_list></name><argument_list>(<argument><expr><name>parsed</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>scalarType</name> <operator>=</operator> <call><name><name>std</name><operator>::</operator><name>get</name><argument_list type="generic">&lt;<argument><expr><literal type="number">1</literal></expr></argument>&gt;</argument_list></name><argument_list>(<argument><expr><name>parsed</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>non_blocking</name> <init>= <expr><call><name><name>std</name><operator>::</operator><name>get</name><argument_list type="generic">&lt;<argument><expr><literal type="number">2</literal></expr></argument>&gt;</argument_list></name><argument_list>(<argument><expr><name>parsed</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>copy</name> <init>= <expr><call><name><name>std</name><operator>::</operator><name>get</name><argument_list type="generic">&lt;<argument><expr><literal type="number">3</literal></expr></argument>&gt;</argument_list></name><argument_list>(<argument><expr><name>parsed</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>std</name><operator>::</operator><name>get</name><argument_list type="generic">&lt;<argument><expr><literal type="number">4</literal></expr></argument>&gt;</argument_list></name><argument_list>(<argument><expr><name>parsed</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <if_stmt><if>if <condition>(<expr><name>device</name> <operator>&amp;&amp;</operator> <call><name><name>device</name><operator>-&gt;</operator><name>is_cuda</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <expr_stmt><expr><call><name><name>torch</name><operator>::</operator><name>utils</name><operator>::</operator><name>cuda_lazy_init</name></name><argument_list>()</argument_list></call></expr>;</expr_stmt>
  </block_content>}</block></if></if_stmt>
  <if_stmt><if>if <condition>(<expr><operator>!</operator><name>device</name> <operator>&amp;&amp;</operator> <operator>!</operator><name>scalarType</name> <operator>&amp;&amp;</operator> <operator>!</operator><name>copy</name> <operator>&amp;&amp;</operator> <operator>!</operator><call><name><name>opt_memory_format</name><operator>.</operator><name>has_value</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <expr_stmt><expr><call><name>Py_INCREF</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
    <return>return <expr><name>self</name></expr>;</return>
  </block_content>}</block></if> <if type="elseif">else if <condition>(<expr><operator>!</operator><name>device</name> <operator>&amp;&amp;</operator> <operator>!</operator><name>scalarType</name></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(
        <argument><expr><call><name>dispatch_to</name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><name>non_blocking</name></expr></argument>, <argument><expr><name>copy</name></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if> <if type="elseif">else if <condition>(<expr><operator>!</operator><name>device</name></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name>dispatch_to</name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><operator>*</operator><name>scalarType</name></expr></argument>, <argument><expr><name>non_blocking</name></expr></argument>, <argument><expr><name>copy</name></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if> <if type="elseif">else if <condition>(<expr><operator>!</operator><name>scalarType</name></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name>dispatch_to</name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><operator>*</operator><name>device</name></expr></argument>, <argument><expr><name>non_blocking</name></expr></argument>, <argument><expr><name>copy</name></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if> <else>else <block>{<block_content>
    <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name>dispatch_to</name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><operator>*</operator><name>device</name></expr></argument>, <argument><expr><operator>*</operator><name>scalarType</name></expr></argument>, <argument><expr><name>non_blocking</name></expr></argument>, <argument><expr><name>copy</name></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></else></if_stmt>
  <expr_stmt><expr><name>Py_RETURN_NONE</name></expr>;</expr_stmt>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<comment type="line">// implemented on the python object b/c arbitrarily nested list not declarable in native_functions.yaml</comment>
<comment type="line">// See: ATen/native/README.md for more context</comment>
<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_tolist</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <macro><name>HANDLE_TH_ERRORS</name></macro>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><literal type="string">"tolist"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>warn</name></name><argument_list>(<argument><expr><literal type="string">"Converting a tensor to a Python list"</literal></expr></argument>, <argument><expr><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>WARN_PYTHON_DATAFLOW</name></name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>self_</name> <init>= <expr><call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <return>return <expr><call><name><name>torch</name><operator>::</operator><name>utils</name><operator>::</operator><name>tensor_to_list</name></name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_type</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list>
<block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(<argument><expr><block>{
    <expr><literal type="string">"type(PyObject* dtype=None, bool non_blocking=False, *, MemoryFormat? memory_format=None)"</literal></expr>,
    <expr><literal type="string">"type(PyObject* dtype=None, bool async=False, *, MemoryFormat? memory_format=None)|deprecated"</literal></expr>
  }</block></expr></argument>)</argument_list></decl>;</decl_stmt>
  <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>self_</name> <operator>=</operator> <call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">3</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <if_stmt><if>if<condition>(<expr><call><name><name>r</name><operator>.</operator><name>has_torch_function</name></name><argument_list>()</argument_list></call></expr>)</condition><block>{<block_content>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>r</name></expr></argument>, <argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>THPVariableClass</name></expr></argument>, <argument><expr><literal type="string">"torch.Tensor"</literal></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>

  <if_stmt><if>if <condition>(<expr><call><name><name>r</name><operator>.</operator><name>isNone</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <return>return <expr><call><name>THPUtils_packString</name><argument_list>(<argument><expr><call><name><name>torch</name><operator>::</operator><name>utils</name><operator>::</operator><name>options_to_string</name></name><argument_list>(<argument><expr><call><name><name>self_</name><operator>.</operator><name>options</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></if></if_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>obj</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>pyobject</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>opt_memory_format</name> <init>= <expr><call><name><name>r</name><operator>.</operator><name>memoryformatOptional</name></name><argument_list>(<argument><expr><literal type="number">2</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <decl_stmt><decl><type><name><name>std</name><operator>::</operator><name>string</name></name></type> <name>type_name</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>bool</name></type> <name>is_dtype</name> <init>= <expr><literal type="boolean">false</literal></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><call><name>PyType_Check</name><argument_list>(<argument><expr><name>obj</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <if_stmt><if>if <condition>(<expr><name>obj</name> <operator>==</operator> <name>THPVariableClass</name></expr>)</condition> <block>{<block_content>
      <expr_stmt><expr><name>type_name</name> <operator>=</operator> <literal type="string">"torch.Tensor"</literal></expr>;</expr_stmt>
    </block_content>}</block></if> <else>else <block>{<block_content>
      <expr_stmt><expr><name>type_name</name> <operator>=</operator> <operator>(</operator><operator>(</operator><name>PyTypeObject</name><operator>*</operator><operator>)</operator><name>obj</name><operator>)</operator><operator>-&gt;</operator><name>tp_name</name></expr>;</expr_stmt>
    </block_content>}</block></else></if_stmt>
  </block_content>}</block></if> <if type="elseif">else if <condition>(<expr><call><name>THPUtils_checkString</name><argument_list>(<argument><expr><name>obj</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <expr_stmt><expr><name>type_name</name> <operator>=</operator> <call><name>THPUtils_unpackString</name><argument_list>(<argument><expr><name>obj</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  </block_content>}</block></if> <if type="elseif">else if <condition>(<expr><call><name>THPDtype_Check</name><argument_list>(<argument><expr><name>obj</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <expr_stmt><expr><name>is_dtype</name> <operator>=</operator> <literal type="boolean">true</literal></expr>;</expr_stmt>
  </block_content>}</block></if> <else>else <block>{<block_content>
    <throw>throw <expr><call><name>TypeError</name><argument_list>(<argument><expr><literal type="string">"dtype must be a type, str, or dtype object"</literal></expr></argument>)</argument_list></call></expr>;</throw>
  </block_content>}</block></else></if_stmt>
  <decl_stmt><decl><type><name>ScalarType</name></type> <name>scalar_type</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>Device</name></type> <name>device</name> <init>= <expr><call><name><name>self_</name><operator>.</operator><name>device</name></name><argument_list>()</argument_list></call></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><name>is_dtype</name></expr>)</condition> <block>{<block_content>
    <expr_stmt><expr><name>scalar_type</name> <operator>=</operator> <call><name><name>r</name><operator>.</operator><name>scalartype</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  </block_content>}</block></if> <else>else <block>{<block_content>
    <decl_stmt><decl><type><name><name>at</name><operator>::</operator><name>TensorOptions</name></name></type> <name>options</name> <init>= <expr><call><name><name>torch</name><operator>::</operator><name>utils</name><operator>::</operator><name>options_from_string</name></name><argument_list>(<argument><expr><name>type_name</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
    <expr_stmt><expr><name>scalar_type</name> <operator>=</operator> <call><name><name>at</name><operator>::</operator><name>typeMetaToScalarType</name></name><argument_list>(<argument><expr><call><name><name>options</name><operator>.</operator><name>dtype</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr>;</expr_stmt>
    <decl_stmt><decl><type><name>auto</name></type> <name>device_type</name> <init>= <expr><call><name><name>options</name><operator>.</operator><name>device</name></name><argument_list>()</argument_list></call><operator>.</operator><call><name>type</name><argument_list>()</argument_list></call></expr></init></decl>;</decl_stmt>
    <if_stmt><if>if <condition>(<expr><name>device_type</name> <operator>!=</operator> <call><name><name>device</name><operator>.</operator><name>type</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
      <expr_stmt><expr><name>device</name> <operator>=</operator> <call><name><name>at</name><operator>::</operator><name>Device</name></name><argument_list>(<argument><expr><name>device_type</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
    </block_content>}</block></if></if_stmt>
  </block_content>}</block></else></if_stmt>
  <if_stmt><if>if <condition>(<expr><call><name><name>device</name><operator>.</operator><name>is_cuda</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <expr_stmt><expr><call><name><name>torch</name><operator>::</operator><name>utils</name><operator>::</operator><name>cuda_lazy_init</name></name><argument_list>()</argument_list></call></expr>;</expr_stmt>
  </block_content>}</block></if></if_stmt>
  <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name>dispatch_to</name><argument_list>(<argument><expr><name>self_</name></expr></argument>, <argument><expr><name>device</name></expr></argument>, <argument><expr><name>scalar_type</name></expr></argument>, <comment type="block">/*non_blocking=*/</comment> <argument><expr><call><name><name>r</name><operator>.</operator><name>toBool</name></name><argument_list>(<argument><expr><literal type="number">1</literal></expr></argument>)</argument_list></call></expr></argument>, <comment type="block">/*copy=*/</comment> <argument><expr><literal type="boolean">false</literal></expr></argument>, <argument><expr><name>opt_memory_format</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<comment type="line">// generated methods start here</comment>

<macro><name>$</name></macro><block>{<block_content><expr><name>py_methods</name></expr></block_content>}</block>

<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>THPVariable_bool_scalar</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <if_stmt><if>if <condition>(<expr><call><name>check_has_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <macro><name>HANDLE_TH_ERRORS</name></macro>
    <return>return <expr><call><name>handle_torch_function</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><literal type="string">"__bool__"</literal></expr></argument>, <argument><expr><name>args</name></expr></argument>)</argument_list></call></expr>;</return>
    <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><call><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>warn</name></name><argument_list>(<argument><expr><literal type="string">"Converting a tensor to a Python boolean"</literal></expr></argument>, <argument><expr><name><name>jit</name><operator>::</operator><name>tracer</name><operator>::</operator><name>WARN_PYTHON_DATAFLOW</name></name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <return>return <expr><call><name>THPVariable_is_nonzero</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>)</argument_list></call></expr>;</return>
</block_content>}</block></function>

<comment type="line">// Wrapper converts a raised TypeError into returning NotImplemented</comment>
<comment type="line">// Used to implement binary arithmetic operators</comment>
<template>template <parameter_list>&lt;<parameter><type><name>PyObject</name><modifier>*</modifier></type> (*Func</parameter>)</parameter_list>(<parameter><type><name>PyObject</name><modifier>*</modifier></type></parameter><operator>,</operator> <parameter><type><name>PyObject</name><modifier>*</modifier></type></parameter><operator>,</operator> <parameter><type><name>PyObject</name><modifier>*</modifier></type>)</parameter>&gt;
<function><type><specifier>static</specifier> <name>PyObject</name> <modifier>*</modifier></type> <name>TypeError_to_NotImplemented_</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list> <block>{<block_content>

  <decl_stmt><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>ret</name> <init>= <expr><call><name>Func</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><operator>!</operator><name>ret</name> <operator>&amp;&amp;</operator> <call><name>PyErr_ExceptionMatches</name><argument_list>(<argument><expr><name>PyExc_TypeError</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <expr_stmt><expr><call><name>PyErr_Clear</name><argument_list>()</argument_list></call></expr>;</expr_stmt>
    <expr_stmt><expr><call><name>Py_INCREF</name><argument_list>(<argument><expr><name>Py_NotImplemented</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
    <expr_stmt><expr><name>ret</name> <operator>=</operator> <name>Py_NotImplemented</name></expr>;</expr_stmt>
  </block_content>}</block></if></if_stmt>
  <return>return <expr><name>ret</name></expr>;</return>
</block_content>}</block></function></template>

<comment type="line">// set_ has to be defined in the template because the c10::Storage object</comment>
<comment type="line">// does not have a type, and we need to make sure the Python storage object's</comment>
<comment type="line">// type matches the tensor's type</comment>
<function><type><specifier>static</specifier> <name>PyObject</name><modifier>*</modifier></type> <name>THPVariable_set_</name><parameter_list>(
    <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>self_</name></decl></parameter>,
    <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>,
    <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>const</specifier> <name>Tensor</name><modifier>&amp;</modifier></type> <name>self</name> <init>= <expr><call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>self_</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <decl_stmt><decl><type><specifier>static</specifier> <name>PythonArgParser</name></type> <name>parser</name><argument_list>(
      <argument><expr><block>{
          <expr><literal type="string">"set_()"</literal></expr>,
          <expr><literal type="string">"set_(Storage source)"</literal></expr>,
          <expr><literal type="string">"set_(Storage source, SymInt storage_offset, SymIntArrayRef size, SymIntArrayRef stride=None)"</literal></expr>,
          <expr><literal type="string">"set_(Tensor source)"</literal></expr>,
          <expr><literal type="string">"set_(Tensor source, SymInt storage_offset, SymIntArrayRef size, SymIntArrayRef stride=None)"</literal></expr>,
      }</block></expr></argument>,
      <comment type="block">/*traceable=*/</comment><argument><expr><literal type="boolean">false</literal></expr></argument>)</argument_list></decl>;</decl_stmt>

  <decl_stmt><decl><type><name><name>ParsedArgs</name><argument_list type="generic">&lt;<argument><expr><literal type="number">4</literal></expr></argument>&gt;</argument_list></name></type> <name>parsed_args</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>_r</name> <init>= <expr><call><name><name>parser</name><operator>.</operator><name>parse</name></name><argument_list>(<argument><expr><name>args</name></expr></argument>, <argument><expr><name>kwargs</name></expr></argument>, <argument><expr><name>parsed_args</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <switch>switch <condition>(<expr><name><name>_r</name><operator>.</operator><name>idx</name></name></expr>)</condition> <block>{<block_content>
    <case>case <expr><literal type="number">0</literal></expr>:</case> <block>{<block_content>
      <comment type="line">// aten::set_(Tensor(a!) self) -&gt; Tensor(a!)</comment>
      <decl_stmt><decl><type><name>auto</name></type> <name>dispatch_set_</name> <init>= <expr><lambda><capture>[]</capture><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name><modifier>&amp;</modifier></type> <name>self</name></decl></parameter>)</parameter_list> -&gt; <type><name>Tensor</name></type> <block>{<block_content>
        <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
        <return>return <expr><call><name><name>self</name><operator>.</operator><name>set_</name></name><argument_list>()</argument_list></call></expr>;</return>
      </block_content>}</block></lambda></expr></init></decl>;</decl_stmt>
      <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name>dispatch_set_</name><argument_list>(<argument><expr><name>self</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
    </block_content>}</block>
    <case>case <expr><literal type="number">1</literal></expr>:</case> <block>{<block_content>
      <comment type="line">// aten::set_.source_Storage(Tensor(a!) self, Storage source) -&gt;</comment>
      <comment type="line">// Tensor(a!)</comment>
      <decl_stmt><decl><type><name><name>at</name><operator>::</operator><name>ScalarType</name></name></type> <name>storage_scalar_type</name></decl>;</decl_stmt>
      <decl_stmt><decl><type><name>bool</name></type> <name>is_typed_storage</name> <init>= <expr><literal type="boolean">true</literal></expr></init></decl>;</decl_stmt>
      <decl_stmt><decl><type><name><name>at</name><operator>::</operator><name>Storage</name></name></type> <name>storage</name> <init>= <expr><call><name><name>_r</name><operator>.</operator><name>storage</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>, <argument><expr><name>storage_scalar_type</name></expr></argument>, <argument><expr><name>is_typed_storage</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
      <expr_stmt><expr><call><name>TORCH_CHECK</name><argument_list>(<argument><expr><name>storage_scalar_type</name> <operator>==</operator> <call><name><name>self</name><operator>.</operator><name>dtype</name></name><argument_list>()</argument_list></call> <operator>||</operator> <operator>!</operator><name>is_typed_storage</name></expr></argument>,
        <argument><expr><literal type="string">"Expected a Storage of type "</literal></expr></argument>, <argument><expr><call><name><name>self</name><operator>.</operator><name>dtype</name></name><argument_list>()</argument_list></call></expr></argument>,
        <argument><expr><literal type="string">" or an UntypedStorage, but got type "</literal></expr></argument>, <argument><expr><name>storage_scalar_type</name></expr></argument>,
        <argument><expr><literal type="string">" for argument 1 'storage'"</literal></expr></argument>)</argument_list></call></expr>;</expr_stmt>
      <decl_stmt><decl><type><name>auto</name></type> <name>dispatch_set_</name> <init>= <expr><lambda><capture>[]</capture><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name><modifier>&amp;</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>Storage</name></type> <name>source</name></decl></parameter>)</parameter_list> -&gt; <type><name>Tensor</name></type> <block>{<block_content>
        <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
        <return>return <expr><call><name><name>self</name><operator>.</operator><name>set_</name></name><argument_list>(<argument><expr><name>source</name></expr></argument>)</argument_list></call></expr>;</return>
      </block_content>}</block></lambda></expr></init></decl>;</decl_stmt>
      <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name>dispatch_set_</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><name>storage</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
    </block_content>}</block>
    <case>case <expr><literal type="number">2</literal></expr>:</case> <block>{<block_content>
      <comment type="line">// aten::set_.source_Storage_storage_offset(Tensor(a!) self, Storage</comment>
      <comment type="line">// source, int storage_offset, int[] size, int[] stride=[]) -&gt; Tensor(a!)</comment>
      <decl_stmt><decl><type><name><name>at</name><operator>::</operator><name>ScalarType</name></name></type> <name>storage_scalar_type</name></decl>;</decl_stmt>
      <decl_stmt><decl><type><name>bool</name></type> <name>is_typed_storage</name> <init>= <expr><literal type="boolean">true</literal></expr></init></decl>;</decl_stmt>
      <decl_stmt><decl><type><name><name>at</name><operator>::</operator><name>Storage</name></name></type> <name>storage</name> <init>= <expr><call><name><name>_r</name><operator>.</operator><name>storage</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>, <argument><expr><name>storage_scalar_type</name></expr></argument>, <argument><expr><name>is_typed_storage</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
      <expr_stmt><expr><call><name>TORCH_CHECK</name><argument_list>(<argument><expr><name>storage_scalar_type</name> <operator>==</operator> <call><name><name>self</name><operator>.</operator><name>dtype</name></name><argument_list>()</argument_list></call> <operator>||</operator> <operator>!</operator><name>is_typed_storage</name></expr></argument>,
        <argument><expr><literal type="string">"Expected a Storage of type "</literal></expr></argument>, <argument><expr><call><name><name>self</name><operator>.</operator><name>dtype</name></name><argument_list>()</argument_list></call></expr></argument>,
        <argument><expr><literal type="string">" or an UntypedStorage, but got type "</literal></expr></argument>, <argument><expr><name>storage_scalar_type</name></expr></argument>,
        <argument><expr><literal type="string">" for argument 1 'storage'"</literal></expr></argument>)</argument_list></call></expr>;</expr_stmt>
      <decl_stmt><decl><type><name>auto</name></type> <name>dispatch_set_</name> <init>= <expr><lambda><capture>[]</capture><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name><modifier>&amp;</modifier></type> <name>self</name></decl></parameter>,
                              <parameter><decl><type><name>Storage</name></type> <name>source</name></decl></parameter>,
                              <parameter><decl><type><name><name>c10</name><operator>::</operator><name>SymInt</name></name></type> <name>storage_offset</name></decl></parameter>,
                              <parameter><decl><type><name><name>c10</name><operator>::</operator><name>SymIntArrayRef</name></name></type> <name>size</name></decl></parameter>,
                              <parameter><decl><type><name><name>c10</name><operator>::</operator><name>SymIntArrayRef</name></name></type> <name>stride</name></decl></parameter>)</parameter_list> -&gt; <type><name>Tensor</name></type> <block>{<block_content>
        <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
        <return>return <expr><call><name><name>self</name><operator>.</operator><name>set__symint</name></name><argument_list>(<argument><expr><name>source</name></expr></argument>, <argument><expr><name>storage_offset</name></expr></argument>, <argument><expr><name>size</name></expr></argument>, <argument><expr><name>stride</name></expr></argument>)</argument_list></call></expr>;</return>
      </block_content>}</block></lambda></expr></init></decl>;</decl_stmt>
      <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name>dispatch_set_</name><argument_list>(
          <argument><expr><name>self</name></expr></argument>, <argument><expr><name>storage</name></expr></argument>, <argument><expr><call><name><name>_r</name><operator>.</operator><name>toSymInt</name></name><argument_list>(<argument><expr><literal type="number">1</literal></expr></argument>)</argument_list></call></expr></argument>, <argument><expr><call><name><name>_r</name><operator>.</operator><name>symintlist</name></name><argument_list>(<argument><expr><literal type="number">2</literal></expr></argument>)</argument_list></call></expr></argument>, <argument><expr><call><name><name>_r</name><operator>.</operator><name>symintlist</name></name><argument_list>(<argument><expr><literal type="number">3</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
    </block_content>}</block>
    <case>case <expr><literal type="number">3</literal></expr>:</case> <block>{<block_content>
      <comment type="line">// aten::set_.source_Tensor(Tensor(a!) self, Tensor source) -&gt; Tensor(a!)</comment>
      <decl_stmt><decl><type><name>auto</name></type> <name>dispatch_set_</name> <init>= <expr><lambda><capture>[]</capture><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name><modifier>&amp;</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><specifier>const</specifier> <name>Tensor</name><modifier>&amp;</modifier></type> <name>source</name></decl></parameter>)</parameter_list> -&gt; <type><name>Tensor</name></type> <block>{<block_content>
        <expr_stmt><expr><call><name>TORCH_CHECK</name><argument_list>(<argument><expr><call><name><name>source</name><operator>.</operator><name>dtype</name></name><argument_list>()</argument_list></call> <operator>==</operator> <call><name><name>self</name><operator>.</operator><name>dtype</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><literal type="string">"Could not set tensor of type "</literal></expr></argument>, <argument><expr><call><name><name>source</name><operator>.</operator><name>dtype</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><literal type="string">" to a tensor of type "</literal></expr></argument>, <argument><expr><call><name><name>self</name><operator>.</operator><name>dtype</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr>;</expr_stmt>
        <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
        <return>return <expr><call><name><name>self</name><operator>.</operator><name>set_</name></name><argument_list>(<argument><expr><name>source</name></expr></argument>)</argument_list></call></expr>;</return>
      </block_content>}</block></lambda></expr></init></decl>;</decl_stmt>
      <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name>dispatch_set_</name><argument_list>(<argument><expr><name>self</name></expr></argument>, <argument><expr><call><name><name>_r</name><operator>.</operator><name>tensor</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
    </block_content>}</block>
    <case>case <expr><literal type="number">4</literal></expr>:</case> <block>{<block_content>
      <comment type="line">// aten::set_.source_Tensor_storage_offset(Tensor(a!) self, Tensor</comment>
      <comment type="line">// source, int storage_offset, int[] size, int[] stride=[]) -&gt; Tensor(a!)</comment>
      <decl_stmt><decl><type><name><name>at</name><operator>::</operator><name>Tensor</name></name></type> <name>storage</name> <init>= <expr><call><name><name>_r</name><operator>.</operator><name>tensor</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
      <decl_stmt><decl><type><name>auto</name></type> <name>dispatch_set_</name> <init>= <expr><lambda><capture>[]</capture><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>Tensor</name><modifier>&amp;</modifier></type> <name>self</name></decl></parameter>,
                              <parameter><decl><type><specifier>const</specifier> <name>Tensor</name><modifier>&amp;</modifier></type> <name>source</name></decl></parameter>,
                              <parameter><decl><type><name><name>c10</name><operator>::</operator><name>SymInt</name></name></type> <name>storage_offset</name></decl></parameter>,
                              <parameter><decl><type><name><name>c10</name><operator>::</operator><name>SymIntArrayRef</name></name></type> <name>size</name></decl></parameter>,
                              <parameter><decl><type><name><name>c10</name><operator>::</operator><name>SymIntArrayRef</name></name></type> <name>stride</name></decl></parameter>)</parameter_list> -&gt; <type><name>Tensor</name></type> <block>{<block_content>
        <decl_stmt><decl><type><name><name>pybind11</name><operator>::</operator><name>gil_scoped_release</name></name></type> <name>no_gil</name></decl>;</decl_stmt>
        <return>return <expr><call><name><name>self</name><operator>.</operator><name>set__symint</name></name><argument_list>(<argument><expr><name>source</name></expr></argument>, <argument><expr><name>storage_offset</name></expr></argument>, <argument><expr><name>size</name></expr></argument>, <argument><expr><name>stride</name></expr></argument>)</argument_list></call></expr>;</return>
      </block_content>}</block></lambda></expr></init></decl>;</decl_stmt>
      <return>return <expr><call><name>wrap</name><argument_list>(<argument><expr><call><name>dispatch_set_</name><argument_list>(
          <argument><expr><name>self</name></expr></argument>, <argument><expr><name>storage</name></expr></argument>, <argument><expr><call><name><name>_r</name><operator>.</operator><name>toSymInt</name></name><argument_list>(<argument><expr><literal type="number">1</literal></expr></argument>)</argument_list></call></expr></argument>, <argument><expr><call><name><name>_r</name><operator>.</operator><name>symintlist</name></name><argument_list>(<argument><expr><literal type="number">2</literal></expr></argument>)</argument_list></call></expr></argument>, <argument><expr><call><name><name>_r</name><operator>.</operator><name>symintlist</name></name><argument_list>(<argument><expr><literal type="number">3</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
    </block_content>}</block>
  </block_content>}</block></switch>
  <expr_stmt><expr><name>Py_RETURN_NONE</name></expr>;</expr_stmt>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<comment type="line">// XXX: ops that are bound here are not exposed to the C++ api nor the JIT.</comment>
<comment type="line">// Any new ops added here should be accompanied with a comment why they are not</comment>
<comment type="line">// being registered through native_functions.yaml, and be tagged cpp / JIT</comment>
<decl_stmt><decl><type><name>PyMethodDef</name></type> <name><name>variable_methods</name><index>[]</index></name> <init>= <expr><block>{
  <comment type="line">// These magic methods are all implemented on python object to wrap NotImplementedError</comment>
  <expr><block>{<expr><literal type="string">"__add__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_add</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__radd__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_add</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__iadd__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_add_</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__rmul__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_mul</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__mul__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_mul</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__imul__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_mul_</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__sub__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_sub</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__isub__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_sub_</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__div__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_div</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__truediv__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_div</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__floordiv__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_floor_divide</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__idiv__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_div_</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__ifloordiv__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_floor_divide_</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__mod__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_remainder</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__imod__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_remainder_</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__eq__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_eq</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__ne__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_ne</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__lt__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_lt</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__le__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_le</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__gt__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_gt</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__ge__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_ge</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__rand__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_bitwise_and</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__ror__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_bitwise_or</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__rxor__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_bitwise_xor</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__bool__"</literal></expr>, <expr><name>THPVariable_bool_scalar</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__float__"</literal></expr>, <expr><name>THPVariable_float_scalar</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__complex__"</literal></expr>, <expr><name>THPVariable_complex_scalar</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__int__"</literal></expr>, <expr><name>THPVariable_integral_scalar</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__long__"</literal></expr>, <expr><name>THPVariable_integral_scalar</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__index__"</literal></expr>, <expr><name>THPVariable_index_scalar</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__nonzero__"</literal></expr>, <expr><name>THPVariable_bool_scalar</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__invert__"</literal></expr>, <expr><name>THPVariable_invert</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"__matmul__"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name><name>TypeError_to_NotImplemented_</name><argument_list type="generic">&lt;<argument><expr><name>THPVariable_matmul</name></expr></argument>&gt;</argument_list></name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"_is_view"</literal></expr>, <expr><name>THPVariable__is_view</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"apply_"</literal></expr>, <expr><name>THPVariable_apply_</name></expr>, <expr><name>METH_O</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"bfloat16"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_bfloat16</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"byte"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_byte</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"char"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_char</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"contiguous"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_contiguous</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"copy_"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_copy_</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"cpu"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_cpu</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"cuda"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_cuda</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"xpu"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_xpu</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"ipu"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_ipu</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"data_ptr"</literal></expr>, <expr><name>THPVariable_data_ptr</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"dim"</literal></expr>, <expr><name>THPVariable_dim</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"has_names"</literal></expr>, <expr><name>THPVariable_has_names</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"double"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_double</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"cdouble"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_cdouble</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"element_size"</literal></expr>, <expr><name>THPVariable_element_size</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"float"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_float</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"cfloat"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_cfloat</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"get_device"</literal></expr>, <expr><name>THPVariable_get_device</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"bool"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_bool</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"half"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_half</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"int"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_int</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"is_contiguous"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_is_contiguous</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"item"</literal></expr>, <expr><name>THPVariable_item</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"long"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_long</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"map_"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_map_</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"map2_"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_map2_</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"ndimension"</literal></expr>, <expr><name>THPVariable_dim</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"nelement"</literal></expr>, <expr><name>THPVariable_numel</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"new"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_new</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"new_tensor"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_new_tensor</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"nonzero"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_nonzero</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"numel"</literal></expr>, <expr><name>THPVariable_numel</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"numpy"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_numpy</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"requires_grad_"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_requires_grad_</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"set_"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_set_</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"short"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_short</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"size"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_size</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"_storage"</literal></expr>, <expr><name>THPVariable_storage</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"storage_offset"</literal></expr>, <expr><name>THPVariable_storage_offset</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"stride"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_stride</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"to"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_to</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"tolist"</literal></expr>, <expr><name>THPVariable_tolist</name></expr>, <expr><name>METH_NOARGS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <expr><block>{<expr><literal type="string">"type"</literal></expr>, <expr><call><name>castPyCFunctionWithKeywords</name><argument_list>(<argument><expr><name>THPVariable_type</name></expr></argument>)</argument_list></call></expr>, <expr><name>METH_VARARGS</name> <operator>|</operator> <name>METH_KEYWORDS</name></expr>, <expr><name>NULL</name></expr>}</block></expr>,
  <macro><name>$</name></macro><expr><block>{<expr><name>py_method_defs</name></expr>}</block>
  <block>{<expr><name>NULL</name></expr>}</block></expr>
}</block></expr></init></decl>;</decl_stmt>

}</block></namespace>}</block></namespace> <comment type="line">// namespace torch::autograd</comment>
</unit>
