<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<unit xmlns="http://www.srcML.org/srcML/src" xmlns:cpp="http://www.srcML.org/srcML/cpp" revision="1.0.0" language="C++" filename="F:\python_workplace\pytorch-master\torch\csrc\tensor\python_tensor.cpp" version="211" hash="854cf620f68709960c34d476dff6108ab862d83b"><cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;torch/csrc/tensor/python_tensor.h&gt;</cpp:file></cpp:include>

<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;pybind11/pybind11.h&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;structmember.h&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;torch/csrc/utils/pybind.h&gt;</cpp:file></cpp:include>

<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;torch/csrc/Dtype.h&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;torch/csrc/DynamicTypes.h&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;torch/csrc/Exceptions.h&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;torch/csrc/Layout.h&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;torch/csrc/autograd/generated/VariableType.h&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;torch/csrc/autograd/python_variable.h&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;torch/csrc/autograd/utils/wrap_outputs.h&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;torch/csrc/autograd/variable.h&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;torch/csrc/utils/cuda_enabled.h&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;torch/csrc/utils/cuda_lazy_init.h&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;torch/csrc/utils/python_strings.h&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;torch/csrc/utils/tensor_new.h&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;torch/csrc/utils/tensor_types.h&gt;</cpp:file></cpp:include>

<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;ATen/ATen.h&gt;</cpp:file></cpp:include>

<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;sstream&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;string&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;type_traits&gt;</cpp:file></cpp:include>
<cpp:include>#<cpp:directive>include</cpp:directive> <cpp:file>&lt;vector&gt;</cpp:file></cpp:include>

<namespace>namespace <name>torch</name> <block>{
<namespace>namespace <name>tensors</name> <block>{

<using>using <namespace>namespace <name>at</name>;</namespace></using>
<using>using <namespace>namespace <name><name>torch</name><operator>::</operator><name>autograd</name></name>;</namespace></using>

<struct>struct <name>PyTensorType</name> <block>{<public type="default">
  <decl_stmt><decl><type><name>PyTypeObject</name></type> <name>py_type</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>THPDtype</name><modifier>*</modifier></type> <name>dtype</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>THPLayout</name><modifier>*</modifier></type> <name>layout</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>bool</name></type> <name>is_cuda</name></decl>;</decl_stmt>
  <comment type="line">// NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,cppcoreguidelines-avoid-magic-numbers,modernize-avoid-c-arrays)</comment>
  <decl_stmt><decl><type><name>char</name></type> <name><name>name</name><index>[<expr><literal type="number">64</literal></expr>]</index></name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>int</name></type> <name>backend</name></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>int</name></type> <name>scalar_type</name></decl>;</decl_stmt>

  <function><type><name>Backend</name></type> <name>get_backend</name><parameter_list>()</parameter_list> <specifier>const</specifier> <block>{<block_content>
    <return>return <expr><cast type="static">static_cast<argument_list type="generic">&lt;<argument><expr><name>Backend</name></expr></argument>&gt;</argument_list><argument_list>(<argument><expr><name>backend</name></expr></argument>)</argument_list></cast></expr>;</return>
  </block_content>}</block></function>

  <function><type><name>DispatchKey</name></type> <name>get_dispatch_key</name><parameter_list>()</parameter_list> <specifier>const</specifier> <block>{<block_content>
    <return>return <expr><call><name>backendToDispatchKey</name><argument_list>(<argument><expr><cast type="static">static_cast<argument_list type="generic">&lt;<argument><expr><name>Backend</name></expr></argument>&gt;</argument_list><argument_list>(<argument><expr><name>backend</name></expr></argument>)</argument_list></cast></expr></argument>)</argument_list></call></expr>;</return>
  </block_content>}</block></function>

  <function><type><name>ScalarType</name></type> <name>get_scalar_type</name><parameter_list>()</parameter_list> <specifier>const</specifier> <block>{<block_content>
    <return>return <expr><cast type="static">static_cast<argument_list type="generic">&lt;<argument><expr><name>ScalarType</name></expr></argument>&gt;</argument_list><argument_list>(<argument><expr><name>scalar_type</name></expr></argument>)</argument_list></cast></expr>;</return>
  </block_content>}</block></function>
</public>}</block>;</struct>

<assert type="static">static_assert<argument_list>(
    <argument><expr><name><name>std</name><operator>::</operator><name>is_standard_layout</name><argument_list type="generic">&lt;<argument><expr><name>PyTensorType</name></expr></argument>&gt;</argument_list><operator>::</operator><name>value</name></name></expr></argument>,
    <argument><expr><literal type="string">"PyTensorType must be standard layout"</literal></expr></argument>)</argument_list>;</assert>

<decl_stmt><decl><type><specifier>static</specifier> <name>Backend</name></type> <name>default_backend</name> <init>= <expr><name><name>Backend</name><operator>::</operator><name>CPU</name></name></expr></init></decl>;</decl_stmt>

<function_decl><type><specifier>static</specifier> <name>void</name></type> <name>py_bind_tensor_types</name><parameter_list>(
    <parameter><decl><type><specifier>const</specifier> <name><name>std</name><operator>::</operator><name>vector</name><argument_list type="generic">&lt;<argument><expr><name>PyTensorType</name><modifier>*</modifier></expr></argument>&gt;</argument_list></name><modifier>&amp;</modifier></type> <name>tensor_types</name></decl></parameter>)</parameter_list>;</function_decl>

<function><type><specifier>static</specifier> <name>TypeError</name></type> <name>unavailable_type</name><parameter_list>(<parameter><decl><type><specifier>const</specifier> <name>PyTensorType</name><modifier>&amp;</modifier></type> <name>type</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <return>return <expr><call><name>TypeError</name><argument_list>(
      <argument><expr><literal type="string">"type %s not available. Torch not compiled with CUDA enabled."</literal></expr></argument>,
      <argument><expr><name><name>type</name><operator>.</operator><name>name</name></name></expr></argument>)</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>PyObject</name><modifier>*</modifier></type> <name>Tensor_new</name><parameter_list>(
    <parameter><decl><type><name>PyTypeObject</name><modifier>*</modifier></type> <name>type</name></decl></parameter>,
    <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>args</name></decl></parameter>,
    <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>kwargs</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>auto</specifier><modifier>&amp;</modifier></type> <name>tensor_type</name> <init>= <expr><operator>*</operator><operator>(</operator><operator>(</operator><name>PyTensorType</name><operator>*</operator><operator>)</operator><name>type</name><operator>)</operator></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><name><name>tensor_type</name><operator>.</operator><name>is_cuda</name></name> <operator>&amp;&amp;</operator> <operator>!</operator><call><name><name>torch</name><operator>::</operator><name>utils</name><operator>::</operator><name>cuda_enabled</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name>unavailable_type</name><argument_list>(<argument><expr><name>tensor_type</name></expr></argument>)</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
  <return>return <expr><call><name>THPVariable_Wrap</name><argument_list>(<argument><expr><call><name><name>torch</name><operator>::</operator><name>utils</name><operator>::</operator><name>legacy_tensor_ctor</name></name><argument_list>(
      <argument><expr><call><name><name>tensor_type</name><operator>.</operator><name>get_dispatch_key</name></name><argument_list>()</argument_list></call></expr></argument>,
      <argument><expr><call><name><name>tensor_type</name><operator>.</operator><name>get_scalar_type</name></name><argument_list>()</argument_list></call></expr></argument>,
      <argument><expr><name>args</name></expr></argument>,
      <argument><expr><name>kwargs</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<comment type="line">// TODO: Deprecate this instancecheck entirely.  It's here to make</comment>
<comment type="line">// instanceof(t, torch.FloatTensor) work, but we are not going to keep</comment>
<comment type="line">// adding torch.QuantizedIntTensor classes for every new tensor type</comment>
<comment type="line">// we add...</comment>
<function><type><specifier>static</specifier> <name>PyObject</name><modifier>*</modifier></type> <name>Tensor_instancecheck</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>_self</name></decl></parameter>, <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>arg</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>HANDLE_TH_ERRORS</name>
  <specifier>auto</specifier></type> <name>self</name> <init>= <expr><operator>(</operator><name>PyTensorType</name><operator>*</operator><operator>)</operator><name>_self</name></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><call><name>THPVariable_Check</name><argument_list>(<argument><expr><name>arg</name></expr></argument>)</argument_list></call></expr>)</condition> <block>{<block_content>
    <decl_stmt><decl><type><specifier>const</specifier> <specifier>auto</specifier><modifier>&amp;</modifier></type> <name>var</name> <init>= <expr><call><name>THPVariable_Unpack</name><argument_list>(<argument><expr><name>arg</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
    <comment type="line">// NB: This is a little unfortunate, in that if I do an isinstance check</comment>
    <comment type="line">// against torch.cuda.FloatTensor, this will immediately initialize CUDA.</comment>
    <comment type="line">// I originally thought that it would not be possible for aten_type_ to</comment>
    <comment type="line">// be nullptr if you had a tensor of some type, in which case you can</comment>
    <comment type="line">// skip initializing aten_type(), but TestAutograd.test_type_conversions</comment>
    <comment type="line">// seems to violate this property (for whatever reason.)</comment>
    <comment type="line">//</comment>
    <comment type="line">// TODO: Stop using legacyExtractDispatchKey here (probably need to build</comment>
    <comment type="line">// in instanceof checking to Tensor class itself)</comment>
    <if_stmt><if>if <condition>(<expr><call><name>legacyExtractDispatchKey</name><argument_list>(<argument><expr><call><name><name>var</name><operator>.</operator><name>key_set</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call> <operator>==</operator> <call><name><name>self</name><operator>-&gt;</operator><name>get_dispatch_key</name></name><argument_list>()</argument_list></call> <operator>&amp;&amp;</operator>
        <call><name><name>var</name><operator>.</operator><name>scalar_type</name></name><argument_list>()</argument_list></call> <operator>==</operator> <cast type="static">static_cast<argument_list type="generic">&lt;<argument><expr><name>ScalarType</name></expr></argument>&gt;</argument_list><argument_list>(<argument><expr><name><name>self</name><operator>-&gt;</operator><name>scalar_type</name></name></expr></argument>)</argument_list></cast></expr>)</condition> <block>{<block_content>
      <expr_stmt><expr><name>Py_RETURN_TRUE</name></expr>;</expr_stmt>
    </block_content>}</block></if></if_stmt>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><name>Py_RETURN_FALSE</name></expr>;</expr_stmt>
  <expr_stmt><expr><name>END_HANDLE_TH_ERRORS</name></expr></expr_stmt>
</block_content>}</block></function>

<function><type><name>PyObject</name><modifier>*</modifier></type> <name>Tensor_dtype</name><parameter_list>(<parameter><decl><type><name>PyTensorType</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>void</name><modifier>*</modifier></type> <name>unused</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <return>return <expr><call><name><name>torch</name><operator>::</operator><name>autograd</name><operator>::</operator><name>utils</name><operator>::</operator><name>wrap</name></name><argument_list>(<argument><expr><name><name>self</name><operator>-&gt;</operator><name>dtype</name></name></expr></argument>)</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><name>PyObject</name><modifier>*</modifier></type> <name>Tensor_layout</name><parameter_list>(<parameter><decl><type><name>PyTensorType</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>void</name><modifier>*</modifier></type> <name>unused</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <return>return <expr><call><name><name>torch</name><operator>::</operator><name>autograd</name><operator>::</operator><name>utils</name><operator>::</operator><name>wrap</name></name><argument_list>(<argument><expr><name><name>self</name><operator>-&gt;</operator><name>layout</name></name></expr></argument>)</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><name>PyObject</name><modifier>*</modifier></type> <name>Tensor_is_cuda</name><parameter_list>(<parameter><decl><type><name>PyTensorType</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>void</name><modifier>*</modifier></type> <name>unused</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <if_stmt><if>if <condition>(<expr><name><name>self</name><operator>-&gt;</operator><name>is_cuda</name></name></expr>)</condition> <block>{<block_content>
    <expr_stmt><expr><name>Py_RETURN_TRUE</name></expr>;</expr_stmt>
  </block_content>}</block></if> <else>else <block>{<block_content>
    <expr_stmt><expr><name>Py_RETURN_FALSE</name></expr>;</expr_stmt>
  </block_content>}</block></else></if_stmt>
</block_content>}</block></function>

<function><type><name>PyObject</name><modifier>*</modifier></type> <name>Tensor_is_sparse</name><parameter_list>(<parameter><decl><type><name>PyTensorType</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>void</name><modifier>*</modifier></type> <name>unused</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <if_stmt><if>if <condition>(<expr><name><name>self</name><operator>-&gt;</operator><name>layout</name><operator>-&gt;</operator><name>layout</name></name> <operator>==</operator> <name><name>at</name><operator>::</operator><name>Layout</name><operator>::</operator><name>Strided</name></name></expr>)</condition> <block>{<block_content>
    <expr_stmt><expr><name>Py_RETURN_FALSE</name></expr>;</expr_stmt>
  </block_content>}</block></if> <else>else <block>{<block_content>
    <expr_stmt><expr><name>Py_RETURN_TRUE</name></expr>;</expr_stmt>
  </block_content>}</block></else></if_stmt>
</block_content>}</block></function>

<function><type><name>PyObject</name><modifier>*</modifier></type> <name>Tensor_is_sparse_csr</name><parameter_list>(<parameter><decl><type><name>PyTensorType</name><modifier>*</modifier></type> <name>self</name></decl></parameter>, <parameter><decl><type><name>void</name><modifier>*</modifier></type> <name>unused</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <if_stmt><if>if <condition>(<expr><name><name>self</name><operator>-&gt;</operator><name>layout</name><operator>-&gt;</operator><name>layout</name></name> <operator>==</operator> <name><name>at</name><operator>::</operator><name>Layout</name><operator>::</operator><name>SparseCsr</name></name></expr>)</condition> <block>{<block_content>
    <expr_stmt><expr><name>Py_RETURN_TRUE</name></expr>;</expr_stmt>
  </block_content>}</block></if> <else>else <block>{<block_content>
    <expr_stmt><expr><name>Py_RETURN_FALSE</name></expr>;</expr_stmt>
  </block_content>}</block></else></if_stmt>
</block_content>}</block></function>

<comment type="line">// NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,cppcoreguidelines-avoid-non-const-global-variables,modernize-avoid-c-arrays)</comment>
<decl_stmt><decl><type><specifier>static</specifier> <name><name>struct</name> <name>PyMethodDef</name></name></type> <name><name>metaclass_methods</name><index>[]</index></name> <init>= <expr><block>{
    <expr><block>{<expr><literal type="string">"__instancecheck__"</literal></expr>, <expr><name>Tensor_instancecheck</name></expr>, <expr><name>METH_O</name></expr>, <expr><literal type="null">nullptr</literal></expr>}</block></expr>,
    <expr><block>{<expr><literal type="null">nullptr</literal></expr>}</block></expr>}</block></expr></init></decl>;</decl_stmt>

<typedef>typedef <function_decl><type><name>PyObject</name><modifier>*</modifier></type> (<modifier>*</modifier><name>getter</name>)<parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type></decl></parameter>, <parameter><decl><type><name>void</name><modifier>*</modifier></type></decl></parameter>)</parameter_list>;</function_decl></typedef>

<comment type="line">// NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,cppcoreguidelines-avoid-non-const-global-variables,modernize-avoid-c-arrays)</comment>
<decl_stmt><decl><type><specifier>static</specifier> <name><name>struct</name> <name>PyGetSetDef</name></name></type> <name><name>metaclass_properties</name><index>[]</index></name> <init>= <expr><block>{
    <expr><block>{<expr><literal type="string">"dtype"</literal></expr>, <expr><operator>(</operator><name>getter</name><operator>)</operator><name>Tensor_dtype</name></expr>, <expr><literal type="null">nullptr</literal></expr>, <expr><literal type="null">nullptr</literal></expr>, <expr><literal type="null">nullptr</literal></expr>}</block></expr>,
    <expr><block>{<expr><literal type="string">"layout"</literal></expr>, <expr><operator>(</operator><name>getter</name><operator>)</operator><name>Tensor_layout</name></expr>, <expr><literal type="null">nullptr</literal></expr>, <expr><literal type="null">nullptr</literal></expr>, <expr><literal type="null">nullptr</literal></expr>}</block></expr>,
    <expr><block>{<expr><literal type="string">"is_cuda"</literal></expr>, <expr><operator>(</operator><name>getter</name><operator>)</operator><name>Tensor_is_cuda</name></expr>, <expr><literal type="null">nullptr</literal></expr>, <expr><literal type="null">nullptr</literal></expr>, <expr><literal type="null">nullptr</literal></expr>}</block></expr>,
    <expr><block>{<expr><literal type="string">"is_sparse"</literal></expr>, <expr><operator>(</operator><name>getter</name><operator>)</operator><name>Tensor_is_sparse</name></expr>, <expr><literal type="null">nullptr</literal></expr>, <expr><literal type="null">nullptr</literal></expr>, <expr><literal type="null">nullptr</literal></expr>}</block></expr>,
    <expr><block>{<expr><literal type="string">"is_sparse_csr"</literal></expr>, <expr><operator>(</operator><name>getter</name><operator>)</operator><name>Tensor_is_sparse_csr</name></expr>, <expr><literal type="null">nullptr</literal></expr>, <expr><literal type="null">nullptr</literal></expr>, <expr><literal type="null">nullptr</literal></expr>}</block></expr>,
    <expr><block>{<expr><literal type="null">nullptr</literal></expr>}</block></expr>}</block></expr></init></decl>;</decl_stmt>

<decl_stmt><decl><type><specifier>static</specifier> <name>PyTypeObject</name></type> <name>metaclass</name> <init>= <expr><block>{
    <expr><call><name>PyVarObject_HEAD_INIT</name><argument_list>(<argument><expr><literal type="null">nullptr</literal></expr></argument>, <argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call> <literal type="string">"torch.tensortype"</literal></expr>, <comment type="block">/* tp_name */</comment>
    <expr>sizeof<operator>(</operator><name>PyTypeObject</name><operator>)</operator></expr> <comment type="block">/* tp_basicsize */</comment>
}</block></expr></init></decl>;</decl_stmt>

<function><type><specifier>static</specifier> <name>void</name></type> <name>py_initialize_metaclass</name><parameter_list>(<parameter><decl><type><name>PyTypeObject</name><modifier>&amp;</modifier></type> <name>metaclass</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <expr_stmt><expr><name><name>metaclass</name><operator>.</operator><name>tp_flags</name></name> <operator>=</operator> <name>Py_TPFLAGS_DEFAULT</name> <operator>|</operator> <name>Py_TPFLAGS_BASETYPE</name></expr>;</expr_stmt>
  <expr_stmt><expr><name><name>metaclass</name><operator>.</operator><name>tp_methods</name></name> <operator>=</operator> <name>metaclass_methods</name></expr>;</expr_stmt>
  <expr_stmt><expr><name><name>metaclass</name><operator>.</operator><name>tp_getset</name></name> <operator>=</operator> <name>metaclass_properties</name></expr>;</expr_stmt>
  <expr_stmt><expr><name><name>metaclass</name><operator>.</operator><name>tp_base</name></name> <operator>=</operator> <operator>&amp;</operator><name>PyType_Type</name></expr>;</expr_stmt>
  <if_stmt><if>if <condition>(<expr><call><name>PyType_Ready</name><argument_list>(<argument><expr><operator>&amp;</operator><name>metaclass</name></expr></argument>)</argument_list></call> <operator>&lt;</operator> <literal type="number">0</literal></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
</block_content>}</block></function>

<decl_stmt><decl><type><specifier>static</specifier> <name>PyTypeObject</name></type> <name>tensor_type_prototype</name> <init>= <expr><block>{
    <expr><call><name>PyVarObject_HEAD_INIT</name><argument_list>(<argument><expr><operator>&amp;</operator><name>metaclass</name></expr></argument>, <argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call> <literal type="null">nullptr</literal></expr>, <comment type="block">/* tp_name */</comment>
    <expr>sizeof<operator>(</operator><name>PyTensorType</name><operator>)</operator></expr> <comment type="block">/* tp_basicsize */</comment>
}</block></expr></init></decl>;</decl_stmt>

<function><type><specifier>static</specifier> <name>void</name></type> <name>py_initialize_tensor_type</name><parameter_list>(
    <parameter><decl><type><name>PyTypeObject</name><modifier>&amp;</modifier></type> <name>type</name></decl></parameter>,
    <parameter><decl><type><specifier>const</specifier> <name>char</name><modifier>*</modifier></type> <name>name</name></decl></parameter>,
    <parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>tp_dict</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <comment type="line">// NOTE: we don't use the typical static declaration of PyTypeObject because</comment>
  <comment type="line">// we need to initialize as many types as there are VariableType instances.</comment>
  <comment type="line">// We copy the basic object fields from a prototype definition and initialize</comment>
  <comment type="line">// the remaining fields below.</comment>
  <expr_stmt><expr><call><name>memcpy</name><argument_list>(<argument><expr><operator>&amp;</operator><name>type</name></expr></argument>, <argument><expr><operator>&amp;</operator><name>tensor_type_prototype</name></expr></argument>, <argument><expr><sizeof>sizeof<argument_list>(<argument><expr><name>PyTypeObject</name></expr></argument>)</argument_list></sizeof></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <comment type="line">// Subclassing from torch.&lt;ScalarType&gt;Tensor isn't supported.</comment>
  <comment type="line">// (Py_TPFLAGS_BASETYPE omitted). Subclassing torch.Tensor still allowed.</comment>
  <expr_stmt><expr><name><name>type</name><operator>.</operator><name>tp_flags</name></name> <operator>=</operator> <name>Py_TPFLAGS_DEFAULT</name></expr>;</expr_stmt>
  <expr_stmt><expr><name><name>type</name><operator>.</operator><name>tp_name</name></name> <operator>=</operator> <name>name</name></expr>;</expr_stmt>
  <expr_stmt><expr><name><name>type</name><operator>.</operator><name>tp_new</name></name> <operator>=</operator> <name>Tensor_new</name></expr>;</expr_stmt>
  <if_stmt><if>if <condition>(<expr><call><name>PyType_Ready</name><argument_list>(<argument><expr><operator>&amp;</operator><name>type</name></expr></argument>)</argument_list></call> <operator>&lt;</operator> <literal type="number">0</literal></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
  <if_stmt><if>if <condition>(<expr><call><name>PyDict_Merge</name><argument_list>(<argument><expr><name><name>type</name><operator>.</operator><name>tp_dict</name></name></expr></argument>, <argument><expr><name>tp_dict</name></expr></argument>, <argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call> <operator>&lt;</operator> <literal type="number">0</literal></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <specifier>const</specifier> <name>char</name><modifier>*</modifier></type> <name>get_module</name><parameter_list>(<parameter><decl><type><name>Backend</name></type> <name>backend</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <switch>switch <condition>(<expr><name>backend</name></expr>)</condition> <block>{<block_content>
    <case>case <expr><name><name>Backend</name><operator>::</operator><name>CPU</name></name></expr>:</case>
      <return>return <expr><literal type="string">"torch"</literal></expr>;</return>
    <case>case <expr><name><name>Backend</name><operator>::</operator><name>CUDA</name></name></expr>:</case>
      <return>return <expr><literal type="string">"torch.cuda"</literal></expr>;</return>
    <case>case <expr><name><name>Backend</name><operator>::</operator><name>SparseCPU</name></name></expr>:</case>
      <return>return <expr><literal type="string">"torch.sparse"</literal></expr>;</return>
    <case>case <expr><name><name>Backend</name><operator>::</operator><name>SparseCUDA</name></name></expr>:</case>
      <return>return <expr><literal type="string">"torch.cuda.sparse"</literal></expr>;</return>
    <default>default:</default>
      <expr_stmt><expr><call><name>AT_ERROR</name><argument_list>(<argument><expr><literal type="string">"invalid backend: "</literal></expr></argument>, <argument><expr><call><name>toString</name><argument_list>(<argument><expr><name>backend</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  </block_content>}</block></switch>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name><name>std</name><operator>::</operator><name>string</name></name></type> <name>get_name</name><parameter_list>(<parameter><decl><type><name>Backend</name></type> <name>backend</name></decl></parameter>, <parameter><decl><type><name>ScalarType</name></type> <name>scalarType</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name><name>std</name><operator>::</operator><name>ostringstream</name></name></type> <name>ss</name></decl>;</decl_stmt>
  <expr_stmt><expr><name>ss</name> <operator>&lt;&lt;</operator> <call><name>get_module</name><argument_list>(<argument><expr><name>backend</name></expr></argument>)</argument_list></call> <operator>&lt;&lt;</operator> <literal type="string">"."</literal> <operator>&lt;&lt;</operator> <call><name>toString</name><argument_list>(<argument><expr><name>scalarType</name></expr></argument>)</argument_list></call> <operator>&lt;&lt;</operator> <literal type="string">"Tensor"</literal></expr>;</expr_stmt>
  <return>return <expr><call><name><name>ss</name><operator>.</operator><name>str</name></name><argument_list>()</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>THPObjectPtr</name></type> <name>get_storage_obj</name><parameter_list>(<parameter><decl><type><name>Backend</name></type> <name>backend</name></decl></parameter>, <parameter><decl><type><name>ScalarType</name></type> <name>dtype</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>auto</name></type> <name>module_name</name> <init>= <expr><call><name>get_module</name><argument_list>(<argument><expr><name>backend</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>module_obj</name> <init>= <expr><call><name>THPObjectPtr</name><argument_list>(<argument><expr><call><name>PyImport_ImportModule</name><argument_list>(<argument><expr><name>module_name</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><operator>!</operator><name>module_obj</name></expr>)</condition><block type="pseudo"><block_content>
    <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw></block_content></block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>storage_name</name> <init>= <expr><call><name><name>std</name><operator>::</operator><name>string</name></name><argument_list>(<argument><expr><call><name>toString</name><argument_list>(<argument><expr><name>dtype</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call> <operator>+</operator> <literal type="string">"Storage"</literal></expr></init></decl>;</decl_stmt>
  <decl_stmt><decl><type><name>THPObjectPtr</name></type> <name>storage</name><argument_list>(
      <argument><expr><call><name>PyObject_GetAttrString</name><argument_list>(<argument><expr><call><name><name>module_obj</name><operator>.</operator><name>get</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><call><name><name>storage_name</name><operator>.</operator><name>c_str</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><operator>!</operator><call><name><name>storage</name><operator>.</operator><name>get</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name>TypeError</name><argument_list>(<argument><expr><literal type="string">"couldn't find storage object %s"</literal></expr></argument>, <argument><expr><call><name><name>storage_name</name><operator>.</operator><name>c_str</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
  <return>return <expr><name>storage</name></expr>;</return>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>void</name></type> <name>set_type</name><parameter_list>(
    <parameter><decl><type><name>PyTensorType</name><modifier>&amp;</modifier></type> <name>type_obj</name></decl></parameter>,
    <parameter><decl><type><name>Backend</name></type> <name>backend</name></decl></parameter>,
    <parameter><decl><type><name>ScalarType</name></type> <name>scalarType</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <comment type="line">// This field is lazily initialized from backend and scalar_type</comment>
  <expr_stmt><expr><name><name>type_obj</name><operator>.</operator><name>backend</name></name> <operator>=</operator> <cast type="static">static_cast<argument_list type="generic">&lt;<argument><expr><name>int</name></expr></argument>&gt;</argument_list><argument_list>(<argument><expr><name>backend</name></expr></argument>)</argument_list></cast></expr>;</expr_stmt>
  <expr_stmt><expr><name><name>type_obj</name><operator>.</operator><name>scalar_type</name></name> <operator>=</operator> <cast type="static">static_cast<argument_list type="generic">&lt;<argument><expr><name>int</name></expr></argument>&gt;</argument_list><argument_list>(<argument><expr><name>scalarType</name></expr></argument>)</argument_list></cast></expr>;</expr_stmt>
  <expr_stmt><expr><name><name>type_obj</name><operator>.</operator><name>layout</name></name> <operator>=</operator> <call><name><name>torch</name><operator>::</operator><name>getTHPLayout</name></name><argument_list>(<argument><expr><call><name>layout_from_backend</name><argument_list>(<argument><expr><name>backend</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <expr_stmt><expr><name><name>type_obj</name><operator>.</operator><name>dtype</name></name> <operator>=</operator> <call><name><name>torch</name><operator>::</operator><name>getTHPDtype</name></name><argument_list>(<argument><expr><name>scalarType</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <expr_stmt><expr><name><name>type_obj</name><operator>.</operator><name>is_cuda</name></name> <operator>=</operator>
      <operator>(</operator><name>backend</name> <operator>==</operator> <name><name>at</name><operator>::</operator><name>Backend</name><operator>::</operator><name>CUDA</name></name> <operator>||</operator> <name>backend</name> <operator>==</operator> <name><name>at</name><operator>::</operator><name>Backend</name><operator>::</operator><name>SparseCUDA</name></name><operator>)</operator></expr>;</expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>void</name></type> <name>set_name</name><parameter_list>(<parameter><decl><type><name>PyTensorType</name><modifier>&amp;</modifier></type> <name>type_obj</name></decl></parameter>, <parameter><decl><type><specifier>const</specifier> <name><name>std</name><operator>::</operator><name>string</name></name><modifier>&amp;</modifier></type> <name>name</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>size_t</name></type> <name>n</name> <init>= <expr><sizeof>sizeof<argument_list>(<argument><expr><name><name>type_obj</name><operator>.</operator><name>name</name></name></expr></argument>)</argument_list></sizeof></expr></init></decl>;</decl_stmt>
  <expr_stmt><expr><call><name>strncpy</name><argument_list>(<argument><expr><name><name>type_obj</name><operator>.</operator><name>name</name></name></expr></argument>, <argument><expr><call><name><name>name</name><operator>.</operator><name>c_str</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><name>n</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <expr_stmt><expr><name><name>type_obj</name><operator>.</operator><name>name</name><index>[<expr><name>n</name> <operator>-</operator> <literal type="number">1</literal></expr>]</index></name> <operator>=</operator> <literal type="char">'\0'</literal></expr>;</expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>THPObjectPtr</name></type> <name>get_tensor_dict</name><parameter_list>()</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>auto</name></type> <name>torch</name> <init>= <expr><call><name>THPObjectPtr</name><argument_list>(<argument><expr><call><name>PyImport_ImportModule</name><argument_list>(<argument><expr><literal type="string">"torch"</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><operator>!</operator><name>torch</name></expr>)</condition><block type="pseudo"><block_content>
    <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw></block_content></block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>tensor_class</name> <init>= <expr><call><name>THPObjectPtr</name><argument_list>(<argument><expr><call><name>PyObject_GetAttrString</name><argument_list>(<argument><expr><name>torch</name></expr></argument>, <argument><expr><literal type="string">"Tensor"</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><operator>!</operator><name>tensor_class</name></expr>)</condition><block type="pseudo"><block_content>
    <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw></block_content></block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>tensor_type</name> <init>= <expr><operator>(</operator><name>PyTypeObject</name><operator>*</operator><operator>)</operator><call><name><name>tensor_class</name><operator>.</operator><name>get</name></name><argument_list>()</argument_list></call></expr></init></decl>;</decl_stmt>
  <expr_stmt><expr><call><name>TORCH_CHECK</name><argument_list>(<argument><expr><name><name>tensor_type</name><operator>-&gt;</operator><name>tp_base</name></name></expr></argument>, <argument><expr><literal type="string">"missing base type for Tensor"</literal></expr></argument>)</argument_list></call></expr>;</expr_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>res</name> <init>= <expr><call><name>THPObjectPtr</name><argument_list>(<argument><expr><call><name>PyDict_New</name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><operator>!</operator><name>res</name></expr>)</condition><block type="pseudo"><block_content>
    <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw></block_content></block></if></if_stmt>

  <if_stmt><if>if <condition>(<expr><call><name>PyDict_Merge</name><argument_list>(<argument><expr><call><name><name>res</name><operator>.</operator><name>get</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><name><name>tensor_type</name><operator>-&gt;</operator><name>tp_dict</name></name></expr></argument>, <argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call> <operator>&lt;</operator> <literal type="number">0</literal></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
  <if_stmt><if>if <condition>(<expr><call><name>PyDict_Merge</name><argument_list>(<argument><expr><call><name><name>res</name><operator>.</operator><name>get</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><name><name>tensor_type</name><operator>-&gt;</operator><name>tp_base</name><operator>-&gt;</operator><name>tp_dict</name></name></expr></argument>, <argument><expr><literal type="number">0</literal></expr></argument>)</argument_list></call> <operator>&lt;</operator> <literal type="number">0</literal></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>

  <return>return <expr><name>res</name></expr>;</return>
</block_content>}</block></function>

<comment type="line">// A note about the lifetime of the various PyTensorType: normally</comment>
<comment type="line">// PyTypeObject instances are statically allocated, but we want to create them</comment>
<comment type="line">// dynamically at init time, because their exact number depends on</comment>
<comment type="line">// torch::utils::all_declared_types(). The memory for each PyTensorType is</comment>
<comment type="line">// allocated by initialize_aten_types() and never freed: technically it's a</comment>
<comment type="line">// leak, but it's not a problem since we want them to be alive for the whole</comment>
<comment type="line">// time of the process anyway.</comment>
<comment type="line">//</comment>
<comment type="line">// An alternative is to use a std::vector&lt;PyTensorType&gt; instead, and let</comment>
<comment type="line">// std::vector to manage the lifetime of its items. This is problematic</comment>
<comment type="line">// though, because it means that the memory of PyTensorType is deallocated at</comment>
<comment type="line">// some point during the exit: if by chance we have another global destructor</comment>
<comment type="line">// and/or atexit() function which tries to access the PyTensorTypes, we risk</comment>
<comment type="line">// an use-after-free error. This happens for example if we embed CPython and</comment>
<comment type="line">// call Py_Finalize inside an atexit() function which was registered before</comment>
<comment type="line">// importing torch.</comment>
<decl_stmt><decl><type><specifier>static</specifier> <name><name>std</name><operator>::</operator><name>vector</name><argument_list type="generic">&lt;<argument><expr><name>PyTensorType</name><modifier>*</modifier></expr></argument>&gt;</argument_list></name></type> <name>tensor_types</name></decl>;</decl_stmt>

<function><type><name>void</name></type> <name>set_default_storage_type</name><parameter_list>(<parameter><decl><type><name>Backend</name></type> <name>backend</name></decl></parameter>, <parameter><decl><type><name>ScalarType</name></type> <name>dtype</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>THPObjectPtr</name></type> <name>storage</name> <init>= <expr><call><name>get_storage_obj</name><argument_list>(<argument><expr><name>backend</name></expr></argument>, <argument><expr><name>dtype</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>torch_module</name> <init>= <expr><call><name>THPObjectPtr</name><argument_list>(<argument><expr><call><name>PyImport_ImportModule</name><argument_list>(<argument><expr><literal type="string">"torch"</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><operator>!</operator><name>torch_module</name></expr>)</condition><block type="pseudo"><block_content>
    <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw></block_content></block></if></if_stmt>

  <if_stmt><if>if <condition>(<expr><call><name>PyObject_SetAttrString</name><argument_list>(<argument><expr><call><name><name>torch_module</name><operator>.</operator><name>get</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><literal type="string">"Storage"</literal></expr></argument>, <argument><expr><name>storage</name></expr></argument>)</argument_list></call> <operator>!=</operator> <literal type="number">0</literal></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
</block_content>}</block></function>

<function><type><name>void</name></type> <name>set_default_tensor_type</name><parameter_list>(
    <parameter><decl><type><name><name>c10</name><operator>::</operator><name>optional</name><argument_list type="generic">&lt;<argument><expr><name>Backend</name></expr></argument>&gt;</argument_list></name></type> <name>backend</name></decl></parameter>,
    <parameter><decl><type><name><name>c10</name><operator>::</operator><name>optional</name><argument_list type="generic">&lt;<argument><expr><name>ScalarType</name></expr></argument>&gt;</argument_list></name></type> <name>dtype</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <if_stmt><if>if <condition>(<expr><call><name><name>backend</name><operator>.</operator><name>has_value</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <expr_stmt><expr><call><name>TORCH_CHECK_TYPE</name><argument_list>(
        <argument><expr><operator>*</operator><name>backend</name> <operator>!=</operator> <name><name>Backend</name><operator>::</operator><name>Undefined</name></name></expr></argument>, <argument><expr><literal type="string">"default type cannot be undefined"</literal></expr></argument>)</argument_list></call></expr>;</expr_stmt>
    <expr_stmt><expr><call><name>TORCH_CHECK_TYPE</name><argument_list>(
        <argument><expr><operator>!</operator><call><name>isSparse</name><argument_list>(<argument><expr><operator>*</operator><name>backend</name></expr></argument>)</argument_list></call></expr></argument>,
        <argument><expr><literal type="string">"only dense types are supported as the default type"</literal></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  </block_content>}</block></if></if_stmt>
  <if_stmt><if>if <condition>(<expr><call><name><name>dtype</name><operator>.</operator><name>has_value</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <expr_stmt><expr><call><name>TORCH_CHECK_TYPE</name><argument_list>(
        <argument><expr><call><name><name>at</name><operator>::</operator><name>isFloatingType</name></name><argument_list>(<argument><expr><operator>*</operator><name>dtype</name></expr></argument>)</argument_list></call></expr></argument>,
        <argument><expr><literal type="string">"only floating-point types are supported as the default type"</literal></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  </block_content>}</block></if></if_stmt>

  <comment type="line">// Try setting default storage in python first as it's the only operation that</comment>
  <comment type="line">// can fail</comment>
  <expr_stmt><expr><call><name>set_default_storage_type</name><argument_list>(
      <argument><expr><call><name><name>backend</name><operator>.</operator><name>value_or</name></name><argument_list>(<argument><expr><name>default_backend</name></expr></argument>)</argument_list></call></expr></argument>,
      <argument><expr><call><name><name>dtype</name><operator>.</operator><name>value_or</name></name><argument_list>(<argument><expr><call><name><name>at</name><operator>::</operator><name>get_default_dtype_as_scalartype</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</expr_stmt>

  <if_stmt><if>if <condition>(<expr><call><name><name>dtype</name><operator>.</operator><name>has_value</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <expr_stmt><expr><call><name><name>at</name><operator>::</operator><name>set_default_dtype</name></name><argument_list>(<argument><expr><call><name>scalarTypeToTypeMeta</name><argument_list>(<argument><expr><operator>*</operator><name>dtype</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  </block_content>}</block></if></if_stmt>
  <if_stmt><if>if <condition>(<expr><call><name><name>backend</name><operator>.</operator><name>has_value</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <expr_stmt><expr><name>default_backend</name> <operator>=</operator> <operator>*</operator><name>backend</name></expr>;</expr_stmt>
  </block_content>}</block></if></if_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>void</name></type> <name>initialize_aten_types</name><parameter_list>(<parameter><decl><type><name><name>std</name><operator>::</operator><name>vector</name><argument_list type="generic">&lt;<argument><expr><name>PyTensorType</name><modifier>*</modifier></expr></argument>&gt;</argument_list></name><modifier>&amp;</modifier></type> <name>tensor_types</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <comment type="line">// includes CUDA types even when PyTorch is not built with CUDA</comment>
  <decl_stmt><decl><type><name>auto</name></type> <name>declared_types</name> <init>= <expr><call><name><name>torch</name><operator>::</operator><name>utils</name><operator>::</operator><name>all_declared_types</name></name><argument_list>()</argument_list></call></expr></init></decl>;</decl_stmt>
  <expr_stmt><expr><call><name><name>tensor_types</name><operator>.</operator><name>resize</name></name><argument_list>(<argument><expr><call><name><name>declared_types</name><operator>.</operator><name>size</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr>;</expr_stmt>

  <for>for <control>(<init><decl><type><name>size_t</name></type> <name>i</name> <init>= <expr><literal type="number">0</literal></expr></init></decl>, <decl><type ref="prev"/><name>end</name> <init>= <expr><call><name><name>declared_types</name><operator>.</operator><name>size</name></name><argument_list>()</argument_list></call></expr></init></decl>;</init> <condition><expr><name>i</name> <operator>!=</operator> <name>end</name></expr>;</condition> <incr><expr><name>i</name><operator>++</operator></expr></incr>)</control> <block>{<block_content>
    <expr_stmt><expr><name><name>tensor_types</name><index>[<expr><name>i</name></expr>]</index></name> <operator>=</operator> <operator>new</operator> <call><name>PyTensorType</name><argument_list>()</argument_list></call></expr>;</expr_stmt>
    <expr_stmt><expr><name>auto</name><operator>&amp;</operator> <name>tensor_type</name> <operator>=</operator> <operator>*</operator><name><name>tensor_types</name><index>[<expr><name>i</name></expr>]</index></name></expr>;</expr_stmt>
    <decl_stmt><decl><type><name>Backend</name></type> <name>backend</name> <init>= <expr><name><name>declared_types</name><index>[<expr><name>i</name></expr>]</index></name><operator>.</operator><name>first</name></expr></init></decl>;</decl_stmt>
    <decl_stmt><decl><type><name>ScalarType</name></type> <name>scalar_type</name> <init>= <expr><name><name>declared_types</name><index>[<expr><name>i</name></expr>]</index></name><operator>.</operator><name>second</name></expr></init></decl>;</decl_stmt>
    <expr_stmt><expr><call><name>set_type</name><argument_list>(<argument><expr><name>tensor_type</name></expr></argument>, <argument><expr><name>backend</name></expr></argument>, <argument><expr><name>scalar_type</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
    <expr_stmt><expr><call><name>set_name</name><argument_list>(<argument><expr><name>tensor_type</name></expr></argument>, <argument><expr><call><name>get_name</name><argument_list>(<argument><expr><name>backend</name></expr></argument>, <argument><expr><name>scalar_type</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  </block_content>}</block></for>

  <expr_stmt><expr><call><name>set_default_tensor_type</name><argument_list>(<argument><expr><name><name>Backend</name><operator>::</operator><name>CPU</name></name></expr></argument>, <argument><expr><name><name>ScalarType</name><operator>::</operator><name>Float</name></name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
</block_content>}</block></function>

<function><type><name>void</name></type> <name>initialize_python_bindings</name><parameter_list>()</parameter_list> <block>{<block_content>
  <comment type="line">// Initialize the at::Type* pointers, name, and properties of the PyTensorType</comment>
  <comment type="line">// vector. After this call, the vector must not be resized.</comment>
  <expr_stmt><expr><call><name>initialize_aten_types</name><argument_list>(<argument><expr><name>tensor_types</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>

  <comment type="line">// Initialize the Python metaclass for the torch.FloatTensor, etc. types.</comment>
  <comment type="line">// The metaclass handles __instancecheck__ checks and binds the dtype property</comment>
  <comment type="line">// on the type objects.</comment>
  <expr_stmt><expr><call><name>py_initialize_metaclass</name><argument_list>(<argument><expr><name>metaclass</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>

  <comment type="line">// Get the tp_dict of the Variable class. We copy function definitions</comment>
  <comment type="line">// onto each Tensor type object so that they can be accessed via e.g.</comment>
  <comment type="line">// `torch.FloatTensor.add`.</comment>
  <decl_stmt><decl><type><name>auto</name></type> <name>tensor_dict</name> <init>= <expr><call><name>get_tensor_dict</name><argument_list>()</argument_list></call></expr></init></decl>;</decl_stmt>

  <comment type="line">// Initialize each Python type object torch.FloatTensor, torch.DoubleTensor,</comment>
  <comment type="line">// etc.</comment>
  <for>for <control>(<init><expr><name>auto</name><operator>&amp;</operator> <name>tensor_type</name> <operator>:</operator> <name>tensor_types</name></expr></init>)</control> <block>{<block_content>
    <expr_stmt><expr><call><name>py_initialize_tensor_type</name><argument_list>(
        <argument><expr><name><name>tensor_type</name><operator>-&gt;</operator><name>py_type</name></name></expr></argument>, <argument><expr><name><name>tensor_type</name><operator>-&gt;</operator><name>name</name></name></expr></argument>, <argument><expr><call><name><name>tensor_dict</name><operator>.</operator><name>get</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  </block_content>}</block></for>

  <comment type="line">// Add the type objects to their corresponding modules. e.g. torch.FloatTensor</comment>
  <comment type="line">// is added to the `torch` module as `FloatTensor`. Also add all the type</comment>
  <comment type="line">// objects to the set torch._tensor_classes.</comment>
  <expr_stmt><expr><call><name>py_bind_tensor_types</name><argument_list>(<argument><expr><name>tensor_types</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>void</name></type> <name>py_bind_tensor_types</name><parameter_list>(
    <parameter><decl><type><specifier>const</specifier> <name><name>std</name><operator>::</operator><name>vector</name><argument_list type="generic">&lt;<argument><expr><name>PyTensorType</name><modifier>*</modifier></expr></argument>&gt;</argument_list></name><modifier>&amp;</modifier></type> <name>tensor_types</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>auto</name></type> <name>torch_module</name> <init>= <expr><call><name>THPObjectPtr</name><argument_list>(<argument><expr><call><name>PyImport_ImportModule</name><argument_list>(<argument><expr><literal type="string">"torch"</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><operator>!</operator><name>torch_module</name></expr>)</condition><block type="pseudo"><block_content>
    <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw></block_content></block></if></if_stmt>

  <decl_stmt><decl><type><name>auto</name></type> <name>tensor_classes</name> <init>= <expr><call><name>THPObjectPtr</name><argument_list>(
      <argument><expr><call><name>PyObject_GetAttrString</name><argument_list>(<argument><expr><call><name><name>torch_module</name><operator>.</operator><name>get</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><literal type="string">"_tensor_classes"</literal></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><operator>!</operator><name>tensor_classes</name></expr>)</condition><block type="pseudo"><block_content>
    <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw></block_content></block></if></if_stmt>

  <for>for <control>(<init><expr><name>auto</name><operator>&amp;</operator> <name>tensor_type</name> <operator>:</operator> <name>tensor_types</name></expr></init>)</control> <block>{<block_content>
    <decl_stmt><decl><type><name>auto</name></type> <name>name</name> <init>= <expr><call><name><name>std</name><operator>::</operator><name>string</name></name><argument_list>(<argument><expr><name><name>tensor_type</name><operator>-&gt;</operator><name>name</name></name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
    <decl_stmt><decl><type><name>auto</name></type> <name>idx</name> <init>= <expr><call><name><name>name</name><operator>.</operator><name>rfind</name></name><argument_list>(<argument><expr><literal type="char">'.'</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
    <decl_stmt><decl><type><name>auto</name></type> <name>type_name</name> <init>= <expr><call><name><name>name</name><operator>.</operator><name>substr</name></name><argument_list>(<argument><expr><name>idx</name> <operator>+</operator> <literal type="number">1</literal></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
    <decl_stmt><decl><type><name>auto</name></type> <name>module_name</name> <init>= <expr><call><name><name>name</name><operator>.</operator><name>substr</name></name><argument_list>(<argument><expr><literal type="number">0</literal></expr></argument>, <argument><expr><name>idx</name></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>

    <decl_stmt><decl><type><name>auto</name></type> <name>module_obj</name> <init>= <expr><call><name>THPObjectPtr</name><argument_list>(<argument><expr><call><name>PyImport_ImportModule</name><argument_list>(<argument><expr><call><name><name>module_name</name><operator>.</operator><name>c_str</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
    <if_stmt><if>if <condition>(<expr><operator>!</operator><name>module_obj</name></expr>)</condition><block type="pseudo"><block_content>
      <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw></block_content></block></if></if_stmt>

    <decl_stmt><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>type_obj</name> <init>= <expr><operator>(</operator><name>PyObject</name><operator>*</operator><operator>)</operator><name>tensor_type</name></expr></init></decl>;</decl_stmt>
    <expr_stmt><expr><call><name>Py_INCREF</name><argument_list>(<argument><expr><name>type_obj</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
    <if_stmt><if>if <condition>(<expr><call><name>PyModule_AddObject</name><argument_list>(<argument><expr><call><name><name>module_obj</name><operator>.</operator><name>get</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><call><name><name>type_name</name><operator>.</operator><name>c_str</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><name>type_obj</name></expr></argument>)</argument_list></call> <operator>&lt;</operator> <literal type="number">0</literal></expr>)</condition> <block>{<block_content>
      <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw>
    </block_content>}</block></if></if_stmt>
    <if_stmt><if>if <condition>(<expr><call><name>PySet_Add</name><argument_list>(<argument><expr><call><name><name>tensor_classes</name><operator>.</operator><name>get</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><name>type_obj</name></expr></argument>)</argument_list></call> <operator>&lt;</operator> <literal type="number">0</literal></expr>)</condition> <block>{<block_content>
      <throw>throw <expr><call><name>python_error</name><argument_list>()</argument_list></call></expr>;</throw>
    </block_content>}</block></if></if_stmt>
  </block_content>}</block></for>
</block_content>}</block></function>

<function><type><specifier>static</specifier> <name>bool</name></type> <name>PyTensorType_Check</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>obj</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <decl_stmt><decl><type><name>auto</name></type> <name>it</name> <init>= <expr><call><name><name>std</name><operator>::</operator><name>find_if</name></name><argument_list>(
      <argument><expr><call><name><name>tensor_types</name><operator>.</operator><name>begin</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><call><name><name>tensor_types</name><operator>.</operator><name>end</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><lambda><capture>[<argument><name>obj</name></argument>]</capture><parameter_list>(<parameter><decl><type><name>PyTensorType</name><modifier>*</modifier></type> <name>x</name></decl></parameter>)</parameter_list> <block>{<block_content>
        <return>return <expr><operator>(</operator><name>PyObject</name><operator>*</operator><operator>)</operator><name>x</name> <operator>==</operator> <name>obj</name></expr>;</return>
      </block_content>}</block></lambda></expr></argument>)</argument_list></call></expr></init></decl>;</decl_stmt>
  <return>return <expr><name>it</name> <operator>!=</operator> <call><name><name>tensor_types</name><operator>.</operator><name>end</name></name><argument_list>()</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><name>void</name></type> <name>py_set_default_tensor_type</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>obj</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <comment type="line">// NOLINTNEXTLINE(cppcoreguidelines-init-variables)</comment>
  <expr_stmt><expr><call><name>TORCH_CHECK_TYPE</name><argument_list>(
      <argument><expr><call><name>PyTensorType_Check</name><argument_list>(<argument><expr><name>obj</name></expr></argument>)</argument_list></call></expr></argument>,
      <argument><expr><literal type="string">"invalid type object: only floating-point types are supported as the default type"</literal></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name>PyTensorType</name><modifier>*</modifier></type> <name>type</name> <init>= <expr><operator>(</operator><name>PyTensorType</name><operator>*</operator><operator>)</operator><name>obj</name></expr></init></decl>;</decl_stmt>
  <if_stmt><if>if <condition>(<expr><name><name>type</name><operator>-&gt;</operator><name>is_cuda</name></name> <operator>&amp;&amp;</operator> <operator>!</operator><call><name><name>torch</name><operator>::</operator><name>utils</name><operator>::</operator><name>cuda_enabled</name></name><argument_list>()</argument_list></call></expr>)</condition> <block>{<block_content>
    <throw>throw <expr><call><name>unavailable_type</name><argument_list>(<argument><expr><operator>*</operator><name>type</name></expr></argument>)</argument_list></call></expr>;</throw>
  </block_content>}</block></if></if_stmt>
  <expr_stmt><expr><call><name>set_default_tensor_type</name><argument_list>(<argument><expr><call><name><name>type</name><operator>-&gt;</operator><name>get_backend</name></name><argument_list>()</argument_list></call></expr></argument>, <argument><expr><call><name><name>type</name><operator>-&gt;</operator><name>get_scalar_type</name></name><argument_list>()</argument_list></call></expr></argument>)</argument_list></call></expr>;</expr_stmt>
</block_content>}</block></function>

<function><type><name>void</name></type> <name>py_set_default_dtype</name><parameter_list>(<parameter><decl><type><name>PyObject</name><modifier>*</modifier></type> <name>obj</name></decl></parameter>)</parameter_list> <block>{<block_content>
  <expr_stmt><expr><call><name>TORCH_CHECK_TYPE</name><argument_list>(
      <argument><expr><call><name>THPDtype_Check</name><argument_list>(<argument><expr><name>obj</name></expr></argument>)</argument_list></call></expr></argument>,
      <argument><expr><literal type="string">"invalid dtype object: only floating-point types are supported as the default type"</literal></expr></argument>)</argument_list></call></expr>;</expr_stmt>
  <decl_stmt><decl><type><name>auto</name></type> <name>scalar_type</name> <init>= <expr><operator>(</operator><operator>(</operator><name>THPDtype</name><operator>*</operator><operator>)</operator><name>obj</name><operator>)</operator><operator>-&gt;</operator><name>scalar_type</name></expr></init></decl>;</decl_stmt>
  <expr_stmt><expr><call><name>set_default_tensor_type</name><argument_list>(<comment type="block">/*backend=*/</comment><argument><expr><name><name>c10</name><operator>::</operator><name>nullopt</name></name></expr></argument>, <argument><expr><name>scalar_type</name></expr></argument>)</argument_list></call></expr>;</expr_stmt>
</block_content>}</block></function>

<function><type><name><name>c10</name><operator>::</operator><name>DispatchKey</name></name></type> <name>get_default_dispatch_key</name><parameter_list>()</parameter_list> <block>{<block_content>
  <return>return <expr><call><name>backendToDispatchKey</name><argument_list>(<argument><expr><name>default_backend</name></expr></argument>)</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><name><name>at</name><operator>::</operator><name>Device</name></name></type> <name>get_default_device</name><parameter_list>()</parameter_list> <block>{<block_content>
  <return>return <expr><call><name><name>at</name><operator>::</operator><name>Device</name></name><argument_list>(<argument><expr><call><name><name>c10</name><operator>::</operator><name>backendToDeviceType</name></name><argument_list>(<argument><expr><name>default_backend</name></expr></argument>)</argument_list></call></expr></argument>)</argument_list></call></expr>;</return>
</block_content>}</block></function>

<function><type><name>ScalarType</name></type> <name>get_default_scalar_type</name><parameter_list>()</parameter_list> <block>{<block_content>
  <return>return <expr><call><name>get_default_dtype_as_scalartype</name><argument_list>()</argument_list></call></expr>;</return>
</block_content>}</block></function>

}</block></namespace> <comment type="line">// namespace tensors</comment>
}</block></namespace> <comment type="line">// namespace torch</comment>
</unit>
